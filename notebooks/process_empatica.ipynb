{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['lines.linewidth'] = 0.91\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib qt\n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool, Slider, Select\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.models import Range1d\n",
    "from bokeh.io import export_png\n",
    "from bokeh.models import DatetimeTickFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import cheby1, sosfiltfilt\n",
    "\n",
    "def apply_resample(\n",
    "    *, time, goal_fs=None, time_rs=None, data=(), indices=(), aa_filter=True, fs=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply a resample to a set of data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    time : numpy.ndarray\n",
    "        Array of original timestamps.\n",
    "    goal_fs : float, optional\n",
    "        Desired sampling frequency in Hz.  One of `goal_fs` or `time_rs` must be\n",
    "        provided.\n",
    "    time_rs : numpy.ndarray, optional\n",
    "        Resampled time series to sample to. One of `goal_fs` or `time_rs` must be\n",
    "        provided.\n",
    "    data : tuple, optional\n",
    "        Tuple of arrays to normally downsample using np.interpolation. Must match the\n",
    "        size of `time`. Can handle `None` inputs, and will return an array of zeros\n",
    "        matching the downsampled size.\n",
    "    indices : tuple, optional\n",
    "        Tuple of arrays of indices to downsample.\n",
    "    aa_filter : bool, optional\n",
    "        Apply an anti-aliasing filter before downsampling. Default is True. This\n",
    "        is the same filter as used by :py:function:`scipy.signal.decimate`.\n",
    "        See [1]_ for details. Ignored if upsampling.\n",
    "    fs : {None, float}, optional\n",
    "        Original sampling frequency in Hz. If `goal_fs` is an integer factor\n",
    "        of `fs`, every nth sample will be taken, otherwise `np.np.interp` will be\n",
    "        used. Leave blank to always use `np.np.interp`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    time_rs : numpy.ndarray\n",
    "        Resampled time.\n",
    "    data_rs : tuple, optional\n",
    "        Resampled data, if provided.\n",
    "    indices_rs : tuple, optional\n",
    "        Resampled indices, if provided.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] https://en.wikipedia.org/wiki/Downsampling_(signal_processing)\n",
    "    \"\"\"\n",
    "\n",
    "    def resample(x, factor, t, t_rs):\n",
    "        if (int(factor) == factor) and (factor > 1):\n",
    "            # in case that t_rs is provided and ends earlier than t\n",
    "            n = np.nonzero(t <= t_rs[-1])[0][-1] + 1\n",
    "            return (x[: n : int(factor)],)\n",
    "        else:\n",
    "            if x.ndim == 1:\n",
    "                return (np.interp(t_rs, t, x),)\n",
    "            elif x.ndim == 2:\n",
    "                xrs = np.zeros((t_rs.size, x.shape[1]), dtype=np.float64)\n",
    "                for j in range(x.shape[1]):\n",
    "                    xrs[:, j] = np.interp(t_rs, t, x[:, j])\n",
    "                return (xrs,)\n",
    "\n",
    "    if fs is None:\n",
    "        # compute sampling frequency by hand\n",
    "        fs = 1 / np.mean(np.diff(time[:5000]))\n",
    "\n",
    "    if time_rs is None and goal_fs is None:\n",
    "        raise ValueError(\"One of `time_rs` or `goal_fs` is required.\")\n",
    "\n",
    "    # get resampled time if necessary\n",
    "    if time_rs is None:\n",
    "        if int(fs / goal_fs) == fs / goal_fs and goal_fs < fs:\n",
    "            time_rs = time[:: int(fs / goal_fs)]\n",
    "        else:\n",
    "            time_rs = np.arange(time[0], time[-1], 1 / goal_fs)\n",
    "    else:\n",
    "        goal_fs = 1 / np.mean(np.diff(time_rs[:5000]))\n",
    "        # prevent t_rs from extrapolating\n",
    "        time_rs = time_rs[time_rs <= time[-1]]\n",
    "\n",
    "    # AA filter, if necessary\n",
    "    if (fs / goal_fs) >= 1.0:\n",
    "        sos = cheby1(8, 0.05, 0.8 / (fs / goal_fs), output=\"sos\")\n",
    "    else:\n",
    "        aa_filter = False\n",
    "\n",
    "    # resample data\n",
    "    data_rs = ()\n",
    "\n",
    "    for dat in data:\n",
    "        if dat is None:\n",
    "            data_rs += (None,)\n",
    "        elif dat.ndim in [1, 2]:\n",
    "            data_to_rs = sosfiltfilt(sos, dat, axis=0) if aa_filter else dat\n",
    "            data_rs += resample(data_to_rs, fs / goal_fs, time, time_rs)\n",
    "        else:\n",
    "            raise ValueError(\"Data dimension exceeds 2, or data not understood.\")\n",
    "\n",
    "    # resampling indices\n",
    "    indices_rs = ()\n",
    "    for idx in indices:\n",
    "        if idx is None:\n",
    "            indices_rs += (None,)\n",
    "        elif idx.ndim == 1:\n",
    "            indices_rs += (\n",
    "                np.around(np.interp(time[idx], time_rs, np.arange(time_rs.size))).astype(np.int_),\n",
    "            )\n",
    "        elif idx.ndim == 2:\n",
    "            indices_rs += (np.zeros(idx.shape, dtype=np.int_),)\n",
    "            for i in range(idx.shape[1]):\n",
    "                indices_rs[-1][:, i] = np.around(\n",
    "                    np.interp(\n",
    "                        time[idx[:, i]], time_rs, np.arange(time_rs.size)\n",
    "                    )  # cast to in on insert\n",
    "                )\n",
    "\n",
    "    ret = (time_rs,)\n",
    "    if data_rs != ():\n",
    "        ret += (data_rs,)\n",
    "    if indices_rs != ():\n",
    "        ret += (indices_rs,)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import warn\n",
    "\n",
    "from avro.datafile import DataFileReader\n",
    "from avro.io import DatumReader\n",
    "from numpy import (\n",
    "    round,\n",
    "    arange,\n",
    "    vstack,\n",
    "    ascontiguousarray,\n",
    "    isclose,\n",
    "    full,\n",
    "    argmin,\n",
    "    abs,\n",
    "    nan,\n",
    "    float64,\n",
    ")\n",
    "\n",
    "class ReadEmpaticaAvro():\n",
    "    \"\"\"\n",
    "    Read Empatica data from an avro file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trim_keys : {None, tuple}, optional\n",
    "        Trim keys provided in the `predict` method. Default (None) will not do any trimming.\n",
    "        Trimming of either start or end can be accomplished by providing None in the place\n",
    "        of the key you do not want to trim. If provided, the tuple should be of the form\n",
    "        (start_key, end_key). When provided, trim datetimes will be assumed to be in the\n",
    "        same timezone as the data (ie naive if naive, or in the timezone provided).\n",
    "    resample_to_accel : bool, optional\n",
    "        Resample any additional data streams to match the accelerometer data stream.\n",
    "        Default is True.\n",
    "    \"\"\"\n",
    "\n",
    "    _file = \"file\"\n",
    "    _time = \"time\"\n",
    "    _acc = \"acc\"\n",
    "    _gyro = \"gyro\"\n",
    "    _mag = \"magnet\"\n",
    "    _temp = \"temperature\"\n",
    "    _days = \"day_ends\"\n",
    "\n",
    "    def __init__(self, trim_keys=None, resample_to_bvp=True):\n",
    "        \n",
    "        self.trim_keys = trim_keys\n",
    "        self.resample_to_bvp = resample_to_bvp\n",
    "\n",
    "    def get_accel(self, raw_accel_dict, results_dict, key):\n",
    "        \"\"\"\n",
    "        Get the raw acceleration data from the avro file record.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_accel_dict : dict\n",
    "            The record from the avro file for a raw data stream.\n",
    "        results_dict : dict\n",
    "            Dictionary where the results will go.\n",
    "        key : str\n",
    "            Name for the results in `results_dict`.\n",
    "        \"\"\"\n",
    "        # sampling frequency\n",
    "        fs = raw_accel_dict[\"samplingFrequency\"]\n",
    "\n",
    "        # timestamp start\n",
    "        ts_start = raw_accel_dict[\"timestampStart\"] / 1e6  # convert to seconds\n",
    "\n",
    "        # imu parameters for scaling to actual values\n",
    "        phys_min = raw_accel_dict[\"imuParams\"][\"physicalMin\"]\n",
    "        phys_max = raw_accel_dict[\"imuParams\"][\"physicalMax\"]\n",
    "        dig_min = raw_accel_dict[\"imuParams\"][\"digitalMin\"]\n",
    "        dig_max = raw_accel_dict[\"imuParams\"][\"digitalMax\"]\n",
    "\n",
    "        # raw acceleration data\n",
    "        accel = ascontiguousarray(\n",
    "            vstack((raw_accel_dict[\"x\"], raw_accel_dict[\"y\"], raw_accel_dict[\"z\"])).T\n",
    "        )\n",
    "\n",
    "        # scale the raw acceleration data to actual values\n",
    "        accel = (accel - dig_min) / (dig_max - dig_min) * (\n",
    "            phys_max - phys_min\n",
    "        ) + phys_min\n",
    "\n",
    "        # create the timestamp array using ts_start, fs, and the number of samples\n",
    "        time = arange(ts_start, ts_start + accel.shape[0] / fs, 1 / fs)[\n",
    "            : accel.shape[0]\n",
    "        ]\n",
    "\n",
    "        if time.size != accel.shape[0]:\n",
    "            raise ValueError(\"Time does not have enough samples for accel array\")\n",
    "\n",
    "        # use special names here so we can just update dictionary later for returning\n",
    "        results_dict[key] = {self._time: time, \"fs\": fs, \"values\": accel}\n",
    "    \n",
    "    def get_bvp(self, raw_bvp_dict, results_dict, key):\n",
    "        \"\"\"\n",
    "        Get the raw blood volume pulse data from the avro file record.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_bvp_dict : dict\n",
    "            The record from the avro file for a raw data stream.\n",
    "        results_dict : dict\n",
    "            Dictionary where the results will go.\n",
    "        key : str\n",
    "            Name for the results in `results_dict`.\n",
    "        \"\"\"\n",
    "        # sampling frequency\n",
    "        fs = round(raw_bvp_dict[\"samplingFrequency\"], decimals=3)\n",
    "        # timestamp start\n",
    "        ts_start = raw_bvp_dict[\"timestampStart\"] / 1e6 # convert to seconds\n",
    "\n",
    "        # raw bvp data\n",
    "        bvp = ascontiguousarray(raw_bvp_dict[\"values\"])\n",
    "\n",
    "        # timestamp array\n",
    "        time = arange(ts_start, ts_start + bvp.size / fs, 1 / fs)[: bvp.shape[0]]\n",
    "\n",
    "        if time.size != bvp.shape[0]:\n",
    "            raise ValueError(\"Time does not have enough samples for bvp array\")\n",
    "        \n",
    "        results_dict[key] = {self._time: time, \"fs\": fs, \"bvp\": bvp}\n",
    "\n",
    "    def get_eda(self, raw_dict, results_dict, key):\n",
    "        \"\"\"\n",
    "        Get the raw electrodermal activity data from the avro file record.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_dict : dict\n",
    "            The record from the avro file for a raw data stream.\n",
    "        results_dict : dict\n",
    "            Dictionary where the results will go.\n",
    "        key : str\n",
    "            Name for the results in `results_dict`.\n",
    "        \"\"\"\n",
    "        if not raw_dict[\"values\"]:\n",
    "            return\n",
    "\n",
    "        # sampling frequency\n",
    "        fs = round(raw_dict[\"samplingFrequency\"], decimals=3)\n",
    "        # timestamp start\n",
    "        ts_start = raw_dict[\"timestampStart\"] / 1e6\n",
    "\n",
    "        # raw eda data\n",
    "        eda = ascontiguousarray(raw_dict[\"values\"])\n",
    "\n",
    "        # timestamp array\n",
    "        time = arange(ts_start, ts_start + eda.size / fs, 1 / fs)[: eda.shape[0]]\n",
    "\n",
    "        if time.size != eda.shape[0]:\n",
    "            raise ValueError(\"Time does not have enough samples for eda array\")\n",
    "        \n",
    "        results_dict[key] = {\"time_eda\": time, \"fs_eda\": fs, \"eda\": eda}\n",
    "\n",
    "    def get_temp(self, raw_dict, results_dict, key):\n",
    "        \"\"\"\n",
    "        Get the raw temperature data from the avro file record.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_dict : dict\n",
    "            The record from the avro file for a raw data stream.\n",
    "        results_dict : dict\n",
    "            Dictionary where the results will go.\n",
    "        key : str\n",
    "            Name for the results in `results_dict`.\n",
    "        \"\"\"\n",
    "        if not raw_dict[\"values\"]:\n",
    "            return\n",
    "\n",
    "        # sampling frequency\n",
    "        fs = round(raw_dict[\"samplingFrequency\"], decimals=3)\n",
    "        # timestamp start\n",
    "        ts_start = raw_dict[\"timestampStart\"] / 1e6\n",
    "\n",
    "        # raw temperature data\n",
    "        temp = ascontiguousarray(raw_dict[\"values\"])\n",
    "\n",
    "        # timestamp array\n",
    "        time = arange(ts_start, ts_start + temp.size / fs, 1 / fs)[: temp.shape[0]]\n",
    "\n",
    "        if time.size != temp.shape[0]:\n",
    "            raise ValueError(\"Time does not have enough samples for temp array\")\n",
    "        \n",
    "        results_dict[key] = {\"time_temp\": time, \"fs_temp\": fs, \"temp\": temp}\n",
    "\n",
    "    def get_values_1d(self, raw_dict, results_dict, key):\n",
    "        \"\"\"\n",
    "        Get the raw 1-dimensional values data from the avro file record.\n",
    "        i.e, PPG, EDA, and temperature\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_dict : dict\n",
    "            The record from the avro file for a raw 1-dimensional values data stream.\n",
    "        results_dict : dict\n",
    "            Dictionary where the results will go.\n",
    "        key : str\n",
    "            Name for the results in `results_dict`.\n",
    "        \"\"\"\n",
    "        if not raw_dict[\"values\"]:\n",
    "            return\n",
    "\n",
    "        # sampling frequency\n",
    "        fs = round(raw_dict[\"samplingFrequency\"], decimals=3)\n",
    "        # timestamp start\n",
    "        ts_start = raw_dict[\"timestampStart\"] / 1e6  # convert to seconds\n",
    "\n",
    "        # raw values data\n",
    "        values = ascontiguousarray(raw_dict[\"values\"])\n",
    "\n",
    "        # timestamp array\n",
    "        time = arange(ts_start, ts_start + values.size / fs, 1 / fs)[: values.shape[0]]\n",
    "\n",
    "        if time.size != values.shape[0]:\n",
    "            raise ValueError(f\"Time does not have enough samples for {key} array\")\n",
    "\n",
    "        results_dict[key] = {self._time: time, \"fs\": fs, \"values\": values}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_systolic_peaks(raw_dict, results_dict, key):\n",
    "        \"\"\"\n",
    "        Get the systolic peaks data from the avro file record.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_dict : dict\n",
    "            The record from the avro file for systolic peaks data.\n",
    "        results_dict : dict\n",
    "            Dictionary where the results will go.\n",
    "        key : str\n",
    "            Name for the results in `results_dict`.\n",
    "        \"\"\"\n",
    "        if not raw_dict[\"peaksTimeNanos\"]:\n",
    "            return\n",
    "\n",
    "        peaks = (\n",
    "            ascontiguousarray(raw_dict[\"peaksTimeNanos\"]) / 1e9\n",
    "        )  # convert to seconds\n",
    "\n",
    "        results_dict[key] = {\"values\": peaks}\n",
    "\n",
    "    def get_steps(self, raw_dict, results_dict, key):\n",
    "        \"\"\"\n",
    "        Get the raw steps data from the avro file record.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_dict : dict\n",
    "            The record from the avro file for raw steps data.\n",
    "        results_dict : dict\n",
    "            Dictionary where the results will go.\n",
    "        key : str\n",
    "            Name for the results in `results_dict`.\n",
    "        \"\"\"\n",
    "        if not raw_dict[\"values\"]:\n",
    "            return\n",
    "\n",
    "        # sampling frequency\n",
    "        fs = round(raw_dict[\"samplingFrequency\"], decimals=3)\n",
    "\n",
    "        # timestamp start\n",
    "        ts_start = raw_dict[\"timestampStart\"] / 1e6  # convert to seconds\n",
    "\n",
    "        # raw steps data\n",
    "        steps = ascontiguousarray(raw_dict[\"values\"])\n",
    "\n",
    "        # timestamp array\n",
    "        time = arange(ts_start, ts_start + steps.size / fs, 1 / fs)[: steps.size]\n",
    "\n",
    "        if time.size != steps.size:\n",
    "            raise ValueError(\"Time does not have enough samples for steps array\")\n",
    "\n",
    "        results_dict[key] = {self._time: time, \"fs\": fs, \"values\": steps}\n",
    "\n",
    "    def handle_resampling(self, streams):\n",
    "        \"\"\"\n",
    "        Handle resampling of data streams. Data will be resampled to match the\n",
    "        BVP (Blood Volume Pulse) data stream.\n",
    "        \"\"\"\n",
    "        if \"bvp\" not in streams:\n",
    "            raise ValueError(\"BVP data stream is missing, cannot resample.\")\n",
    "        \n",
    "        # Remove BVP data stream\n",
    "        bvp_dict = streams.pop(\"bvp\")\n",
    "        # Remove Temp data stream\n",
    "        temp_dict = streams.pop(\"temperature\")\n",
    "        # Remove EDA data stream\n",
    "        eda_dict = streams.pop(\"eda\")\n",
    "\n",
    "        # Remove keys that cannot be resampled\n",
    "        rs_streams = {d: streams.pop(d) for d in [\"systolic_peaks\", \"steps\"] if d in streams}\n",
    "        \n",
    "        for name, stream in streams.items():\n",
    "            if stream[\"values\"] is None:\n",
    "                continue\n",
    "            \n",
    "            # Check that the stream doesn't start significantly later than BVP\n",
    "            # if (dt := (stream[\"time\"][0] - bvp_dict[\"time\"][0])) > 1:\n",
    "            #     warn(\n",
    "            #         f\"Data stream {name} starts more than 1 second ({dt}s) after \"\n",
    "            #         f\"the BVP stream. Data will be filled with the first (and \"\n",
    "            #         f\"last) value as needed.\"\n",
    "            #     )\n",
    "            \n",
    "            # Check if resampling is needed\n",
    "            if isclose(stream[\"fs\"], bvp_dict[\"fs\"], atol=1e-3):\n",
    "                new_shape = list(stream[\"values\"].shape)\n",
    "                new_shape[0] = bvp_dict[\"bvp\"].shape[0]\n",
    "                rs_streams[name] = full(new_shape, nan, dtype=float64)\n",
    "                i1 = argmin(abs(bvp_dict[\"time\"] - stream[\"time\"][0]))\n",
    "                i2 = i1 + stream[\"time\"].size\n",
    "                rs_streams[name][i1:i2] = stream[\"values\"][: stream[\"values\"].shape[0] - (i2 - bvp_dict[\"time\"].size)]\n",
    "                rs_streams[name][:i1] = stream[\"values\"][0]\n",
    "                rs_streams[name][i2:] = stream[\"values\"][-1]\n",
    "                continue\n",
    "            \n",
    "            # Resample the stream to match BVP\n",
    "            _, (stream_rs,) = apply_resample(\n",
    "                time=stream[\"time\"],\n",
    "                time_rs=bvp_dict[\"time\"],\n",
    "                data=(stream[\"values\"],),\n",
    "                aa_filter=True,\n",
    "                fs=stream[\"fs\"],\n",
    "            )\n",
    "            rs_streams[name] = stream_rs\n",
    "        \n",
    "        rs_streams.update(bvp_dict)\n",
    "        rs_streams.update(temp_dict)\n",
    "        rs_streams.update(eda_dict)\n",
    "        return rs_streams\n",
    "\n",
    "    def get_datastreams(self, raw_record):\n",
    "        \"\"\"\n",
    "        Extract the various data streams from the raw avro file record.\n",
    "        \"\"\"\n",
    "        fn_map = {\n",
    "            \"accelerometer\": (\"acc\", self.get_accel),\n",
    "            \"eda\": (\"eda\", self.get_eda),\n",
    "            \"temperature\": (\"temperature\", self.get_temp),\n",
    "            \"bvp\": (\"bvp\", self.get_bvp),\n",
    "            \"systolicPeaks\": (\"systolic_peaks\", self.get_systolic_peaks),\n",
    "            \"steps\": (\"steps\", self.get_steps),\n",
    "        }\n",
    "\n",
    "        raw_data_streams = {}\n",
    "        for full_name, (stream_name, fn) in fn_map.items():\n",
    "            fn(raw_record[full_name], raw_data_streams, stream_name)\n",
    "        \n",
    "        if self.resample_to_bvp:\n",
    "            data_streams = self.handle_resampling(raw_data_streams)\n",
    "        else:\n",
    "            data_streams = raw_data_streams.pop(\"bvp\")\n",
    "            data_streams.update(raw_data_streams)\n",
    "        \n",
    "        return data_streams\n",
    "\n",
    "    def read(self, *, file, tz_name=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Read the input .avro file.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file : {path-like, str}\n",
    "            The path to the input file.\n",
    "        tz_name : {None, optional}\n",
    "            IANA time-zone name for the recording location. If not provided, timestamps\n",
    "            will represent local time naively. This means they will not account for\n",
    "            any time changes due to Daylight Saving Time.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        results : dict\n",
    "            Dictionary containing the data streams from the file. See Notes\n",
    "            for different output options.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        There are two output formats, based on if `resample_to_accel` is True or False.\n",
    "        If True, all available data streams except for `systolic_peaks` and `steps`\n",
    "        are resampled to match the accelerometer data stream, which results in their\n",
    "        values being present in the top level of the `results` dictionary, ie\n",
    "        `results['gyro']`, etc.\n",
    "\n",
    "        If False, everything except accelerometer will be present in dictionaries\n",
    "        containing the keys `time`, `fs`, and `values`, and the top level will be these\n",
    "        dictionaries plus the accelerometer data (keys `time`, `fs`, and `accel`).\n",
    "\n",
    "        `systolic_peaks` will always be a dictionary of the form `{'systolic_peaks': array}`.\n",
    "        \"\"\"\n",
    "\n",
    "        reader = DataFileReader(open(file, \"rb\"), DatumReader())\n",
    "        records = []\n",
    "        for record in reader:\n",
    "            records.append(record)\n",
    "        reader.close()\n",
    "\n",
    "        # get the timezone offset\n",
    "        tz_offset = records[0][\"timezone\"]  # in seconds\n",
    "\n",
    "        # as needed, deviceSn, deviceModel\n",
    "\n",
    "        # get the data streams\n",
    "        results = self.get_datastreams(records[0][\"rawData\"])\n",
    "\n",
    "        # update the timestamps to be local. Do this as we don't have an actual\n",
    "        # timezone from the data.\n",
    "        if tz_name is None:\n",
    "            results[\"time\"] += tz_offset\n",
    "            results[\"time_temp\"] += tz_offset\n",
    "            results[\"time_eda\"] += tz_offset\n",
    "\n",
    "            for k in results:\n",
    "                if k == \"time\":\n",
    "                    continue\n",
    "                if (\n",
    "                    isinstance(results[k], dict)\n",
    "                    and \"time\" in results[k]\n",
    "                    and results[k][\"time\"] is not None\n",
    "                ):\n",
    "                    results[k][\"time\"] += tz_offset\n",
    "        # do nothing if we have the time-zone name, the timestamps are already\n",
    "        # UTC\n",
    "\n",
    "        # adjust systolic_peaks\n",
    "        if \"systolic_peaks\" in results:\n",
    "            results[\"systolic_peaks\"][\"values\"] += tz_offset\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "empatica_reader = ReadEmpaticaAvro()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Acc for GGIR\n",
    "~ 2 min for reading and concatenating into a list\n",
    "\n",
    "**With Pandas**\n",
    "\n",
    "- ~ 2.30 min for transforming it into a pd.DataFrame\n",
    "- ~ 1.30 min for saving to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing day 1/8\n",
      "Processing day 2/8\n",
      "Processing day 3/8\n",
      "Processing day 4/8\n",
      "Processing day 5/8\n",
      "Processing day 6/8\n",
      "Processing day 7/8\n",
      "Processing day 8/8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "#### Change the paths below to the location of the data on your machine ####\n",
    "data_path = \"/Users/augenpro/Documents/Empatica/data_sara/data/participant_data/\"\n",
    "save_data_path = \"/Users/augenpro/Documents/Empatica/data_sara/data/GGIR_input/\"\n",
    "#### Change the subject ID and device ID below to the subject and device you want to process ####\n",
    "sub_ID = \"00007\"\n",
    "device_ID = \"3YK3J151VJ\"\n",
    "\n",
    "days = sorted(os.listdir(data_path))\n",
    "days = [day for day in days if day[0] != \".\"] # remove hidden files (needed for MacOS users)\n",
    "\n",
    "acc = []\n",
    "ppg = []\n",
    "temp = []\n",
    "time = []\n",
    "time_temp = []\n",
    "\n",
    "for i, day in enumerate(days):\n",
    "    \n",
    "    print(f\"Processing day {i+1}/{len(days)}\")\n",
    "\n",
    "    folder_day = data_path + day + \"/\" + sub_ID + \"-\" + device_ID + \"/raw_data/v6\"\n",
    "\n",
    "    avro_files = sorted(glob.glob(folder_day + \"/*.avro\"))\n",
    "\n",
    "    for avro_file in avro_files:\n",
    "        \n",
    "        data = empatica_reader.read(file=avro_file)\n",
    "\n",
    "        acc.extend(data[\"acc\"])\n",
    "\n",
    "        ppg.extend(data[\"bvp\"])\n",
    "\n",
    "        temp.extend(data[\"temp\"])\n",
    "        time_temp.extend(data[\"time_temp\"])\n",
    "\n",
    "        time.extend(data[\"time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect non-wear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df = pd.DataFrame(acc, columns=[\"x\", \"y\", \"z\"], index = pd.to_datetime(time, unit=\"s\")).sort_index()\n",
    "ppg_df = pd.DataFrame(ppg, columns=[\"ppg\"], index = pd.to_datetime(time, unit=\"s\")).sort_index()\n",
    "temp_df = pd.DataFrame(temp, columns=[\"temp\"], index = pd.to_datetime(time_temp, unit=\"s\")).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to divide it into portions when the device was in charge\n",
    "\n",
    "t_charge_end = acc_df.index[acc_df.index.to_series().diff().dt.total_seconds() > 60*10]\n",
    "t_charge_start = acc_df.index[np.where(acc_df.index.to_series().diff().dt.total_seconds() > 60*10)[0]-1]\n",
    "t_charge = pd.DataFrame({\"start\": t_charge_start, \"end\": t_charge_end})\n",
    "\n",
    "good_portions = pd.DataFrame(columns=[\"start\", \"end\"])\n",
    "good_portions[\"start\"] = t_charge[\"end\"].iloc[:-1].reset_index(drop=True)\n",
    "good_portions[\"end\"] = t_charge[\"start\"].iloc[1:].reset_index(drop=True)\n",
    "start_first_charge = t_charge[\"start\"].iloc[0]\n",
    "end_last_charge = t_charge[\"end\"].iloc[-1]\n",
    "\n",
    "# Segment the data into portions when the device was not in charge and perform nonwear detection\n",
    "acc_df_portions = [acc_df[:start_first_charge]]\n",
    "ppg_df_portions = [ppg_df[:start_first_charge]]\n",
    "temp_df_portions = [temp_df[:start_first_charge]]\n",
    "\n",
    "for i, row in good_portions.iterrows():\n",
    "\n",
    "    if row[\"end\"] - row[\"start\"] < pd.Timedelta(\"10 min\"): # if the portion is less than 10 minutes, skip it\n",
    "        continue\n",
    "\n",
    "    acc_df_portions.append(acc_df[row[\"start\"]:row[\"end\"]])\n",
    "    ppg_df_portions.append(ppg_df[row[\"start\"]:row[\"end\"]])\n",
    "    temp_df_portions.append(temp_df[row[\"start\"]:row[\"end\"]])\n",
    "\n",
    "acc_df_portions.append(acc_df[end_last_charge:])\n",
    "ppg_df_portions.append(ppg_df[end_last_charge:])\n",
    "temp_df_portions.append(temp_df[end_last_charge:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nonwear.DETACH import nimbaldetach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each of them, perform NW detection\n",
    "acc_df_cleaned = []\n",
    "temp_df_cleaned = []\n",
    "ppg_df_cleaned = []\n",
    "for i, (acc, temp, ppg) in enumerate(zip(acc_df_portions, temp_df_portions, ppg_df_portions)):\n",
    "\n",
    "    start_stop_nw, _ = nimbaldetach(acc['x'].values, acc['y'].values, acc['z'].values, temp[\"temp\"].values, accel_freq=64, temperature_freq=1, quiet=True)\n",
    "\n",
    "    # Remove non-wear periods\n",
    "    for i, row in start_stop_nw.iterrows():\n",
    "        datetime_start_nw = acc.index[row[\"Start Datapoint\"]]\n",
    "        datetime_end_nw = acc.index[row[\"End Datapoint\"]]\n",
    "        acc.loc[datetime_start_nw:datetime_end_nw] = np.nan\n",
    "        temp.loc[datetime_start_nw:datetime_end_nw] = np.nan\n",
    "        ppg.loc[datetime_start_nw:datetime_end_nw] = np.nan\n",
    "\n",
    "    acc_portion = acc.dropna()\n",
    "    temp_portion = temp.dropna()\n",
    "    ppg_portion = ppg.dropna()\n",
    "\n",
    "    plt.figure(figsize=(19, 11))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(acc_portion.resample(\"20s\").mean(), label=\"acc\")\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 1, 2, sharex = plt.subplot(2, 1, 1))\n",
    "    plt.plot(temp_portion.resample(\"30 s\").mean(), label=\"temp\")\n",
    "    plt.legend()\n",
    "\n",
    "    acc_df_cleaned.append(acc_portion)\n",
    "    temp_df_cleaned.append(temp_portion)\n",
    "    ppg_df_cleaned.append(ppg_portion)\n",
    "\n",
    "acc_df_cleaned = pd.concat(acc_df_cleaned)\n",
    "temp_df_cleaned = pd.concat(temp_df_cleaned)\n",
    "ppg_df_cleaned = pd.concat(ppg_df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv for GGIR\n",
    "acc_df.to_csv(save_data_path + \"/acc_new.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x31b6153d0>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(19, 11))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(acc_df.resample(\"30s\").mean())\n",
    "plt.subplot(3, 1, 2, sharex = plt.subplot(3, 1, 1))\n",
    "plt.plot(temp_df.resample(\"30s\").mean())\n",
    "plt.subplot(3, 1, 3, sharex = plt.subplot(3, 1, 1))\n",
    "plt.plot(ppg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x43ae282f0>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(19, 11))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc_df.values)\n",
    "plt.subplot(2, 1, 2, sharex = plt.subplot(2, 1, 1))\n",
    "plt.plot(ppg_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to parquet\n",
    "acc_df.to_parquet(save_data_path + \"/acc.parquet\")\n",
    "temp_df.to_parquet(save_data_path + \"/temp.parquet\")\n",
    "ppg_df.to_parquet(save_data_path + \"/ppg.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GGIR output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_GGIR_path = \"/Users/augenpro/Documents/Empatica/data_sara/data/GGIR_output/output_GGIR_input/results/QC/\"\n",
    "\n",
    "output_GGIR = pd.read_csv(output_GGIR_path + \"part4_nightsummary_sleep_full.csv\")\n",
    "\n",
    "SPT = []\n",
    "\n",
    "for i, day_row in output_GGIR.iterrows():\n",
    "    # Stupid thing to get the correct datetime for segmenting signals into day and night (but no alternatives I guess)\n",
    "    if output_GGIR[\"sleeponset_ts\"].iloc[0][0] == '0':\n",
    "        sleep_onset = pd.to_datetime(str(pd.to_datetime(day_row[\"calendar_date\"]).date() + pd.Timedelta(\"1d\")) + \" \" + day_row[\"sleeponset_ts\"])\n",
    "    else:\n",
    "        sleep_onset = pd.to_datetime(pd.to_datetime(day_row[\"calendar_date\"]).date() + \" \" + day_row[\"sleeponset_ts\"])\n",
    "\n",
    "    wake_onset = pd.to_datetime(str(pd.to_datetime(day_row[\"calendar_date\"]).date() + pd.Timedelta(\"1d\")) + \" \" + day_row[\"wakeup_ts\"])\n",
    "\n",
    "    SPT.append((sleep_onset, wake_onset))\n",
    "\n",
    "start_end_sleep = np.array(SPT).reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[Timestamp('2024-05-21 01:02:45'),\n",
       "        Timestamp('2024-05-21 08:07:30')],\n",
       "       [Timestamp('2024-05-22 00:15:30'),\n",
       "        Timestamp('2024-05-22 05:59:55')],\n",
       "       [Timestamp('2024-05-23 00:15:30'),\n",
       "        Timestamp('2024-05-23 06:10:10')],\n",
       "       [Timestamp('2024-05-25 01:03:10'),\n",
       "        Timestamp('2024-05-25 06:07:55')],\n",
       "       [Timestamp('2024-05-26 23:48:50'),\n",
       "        Timestamp('2024-05-26 09:49:05')],\n",
       "       [Timestamp('2024-05-27 23:39:40'),\n",
       "        Timestamp('2024-05-27 06:14:20')]], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_end_sleep[1,0] = pd.Timestamp('2024-05-22 00:15:30')\n",
    "start_end_sleep[2,0] = pd.Timestamp('2024-05-23 00:15:30')\n",
    "# remove the fourth night\n",
    "start_end_sleep = np.delete(start_end_sleep, 3, axis=0)\n",
    "start_end_sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[Timestamp('2024-05-21 01:02:45'),\n",
       "        Timestamp('2024-05-21 08:07:30')],\n",
       "       [Timestamp('2024-05-22 00:15:30'),\n",
       "        Timestamp('2024-05-22 05:59:55')],\n",
       "       [Timestamp('2024-05-23 00:15:30'),\n",
       "        Timestamp('2024-05-23 06:10:10')],\n",
       "       [Timestamp('2024-05-24 00:30:30'),\n",
       "        Timestamp('2024-05-24 07:40:30')],\n",
       "       [Timestamp('2024-05-26 00:00:30'),\n",
       "        Timestamp('2024-05-26 09:30:30')],\n",
       "       [Timestamp('2024-05-26 23:45:30'),\n",
       "        Timestamp('2024-05-27 06:05:30')]], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_end_sleep[3,0] = pd.Timestamp('2024-05-24 00:30:30')\n",
    "start_end_sleep[3,1] = pd.Timestamp('2024-05-24 07:40:30')\n",
    "start_end_sleep[4,0] = pd.Timestamp('2024-05-26 00:00:30')\n",
    "start_end_sleep[4,1] = pd.Timestamp('2024-05-26 09:30:30')\n",
    "start_end_sleep[5,0] = pd.Timestamp('2024-05-26 23:45:30')\n",
    "start_end_sleep[5,1] = pd.Timestamp('2024-05-27 06:05:30')\n",
    "\n",
    "start_end_sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract night HRV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = \"/Users/augenpro/Documents/Empatica/data_sara/data/GGIR_input/\"\n",
    "\n",
    "ppg_df = pd.read_parquet(parquet_path + \"ppg.parquet\")\n",
    "temp_df = pd.read_parquet(parquet_path + \"temp.parquet\")\n",
    "acc_df = pd.read_parquet(parquet_path + \"acc.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment night data\n",
    "plt.figure(figsize=(19, 11))\n",
    "plt.plot(acc_df.resample(\"1s\").mean())\n",
    "for start_sleep, end_sleep in start_end_sleep:\n",
    "    plt.axvspan(start_sleep, end_sleep, color=\"red\", alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc_SMV(acc_df):\n",
    "    return np.sqrt(acc_df[\"x\"]**2 + acc_df[\"y\"]**2 + acc_df[\"z\"]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sleep.acc_bursts.detect_acc_bursts import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "def ppg_findpeaks_charlton(\n",
    "    signal,\n",
    "    sampling_rate=1000,\n",
    "    show=False,\n",
    "):\n",
    "    \"\"\"Implementation of Charlton et al (2024) MSPTDfast: An Efficient Photoplethysmography\n",
    "    Beat Detection Algorithm. 2024 Computing in Cardiology (CinC), Karlsruhe, Germany,\n",
    "    doi:10.1101/2024.07.18.24310627.\n",
    "    \"\"\"\n",
    "\n",
    "    # Inner functions\n",
    "\n",
    "    def find_m_max(x, N, max_scale, m_max):\n",
    "        \"\"\"Find local maxima scalogram for peaks\n",
    "        \"\"\"\n",
    "\n",
    "        for k in range(1, max_scale + 1):  # scalogram scales\n",
    "            for i in range(k + 2, N - k + 2):\n",
    "                if x[i - 2] > x[i - k - 2] and x[i - 2] > x[i + k - 2]:\n",
    "                    m_max[k - 1, i - 2] = True\n",
    "\n",
    "        return m_max\n",
    "\n",
    "    def find_m_min(x, N, max_scale, m_min):\n",
    "        \"\"\"Find local minima scalogram for onsets\n",
    "        \"\"\"\n",
    "\n",
    "        for k in range(1, max_scale + 1):  # scalogram scales\n",
    "            for i in range(k + 2, N - k + 2):\n",
    "                if x[i - 2] < x[i - k - 2] and x[i - 2] < x[i + k - 2]:\n",
    "                    m_min[k - 1, i - 2] = True\n",
    "\n",
    "        return m_min\n",
    "\n",
    "    def find_lms_using_msptd_approach(max_scale, x, options):\n",
    "        \"\"\"Find local maxima (or minima) scalogram(s) using the\n",
    "        MSPTD approach\n",
    "        \"\"\"\n",
    "\n",
    "        # Setup\n",
    "        N = len(x)\n",
    "\n",
    "        # Find local maxima scalogram (if required)\n",
    "        if options[\"find_pks\"]:\n",
    "            m_max = np.full((max_scale, N), False)  # matrix for maxima\n",
    "            m_max = find_m_max(x, N, max_scale, m_max)\n",
    "        else:\n",
    "            m_max = None\n",
    "\n",
    "        # Find local minima scalogram (if required)\n",
    "        if options[\"find_trs\"]:\n",
    "            m_min = np.full((max_scale, N), False)  # matrix for minima\n",
    "            m_min = find_m_min(x, N, max_scale, m_min)\n",
    "        else:\n",
    "            m_min = None\n",
    "\n",
    "        return m_max, m_min\n",
    "\n",
    "    def downsample(win_sig, ds_factor):\n",
    "        \"\"\"Downsamples signal by picking out every nth sample, where n is\n",
    "        specified by ds_factor\n",
    "        \"\"\"\n",
    "\n",
    "        return win_sig[::ds_factor]\n",
    "\n",
    "    def detect_peaks_and_onsets_using_msptd(signal, fs, options):\n",
    "        \"\"\"Detect peaks and onsets in a PPG signal using a modified MSPTD approach\n",
    "        (where the modifications are those specified in Charlton et al. 2024)\n",
    "        \"\"\"\n",
    "\n",
    "        # Setup\n",
    "        N = len(signal)\n",
    "        L = int(np.ceil(N / 2) - 1)\n",
    "\n",
    "        # Step 0: Don't calculate scales outside the range of plausible HRs\n",
    "\n",
    "        plaus_hr_hz = np.array(options['plaus_hr_bpm']) / 60  # in Hz\n",
    "        init_scales = np.arange(1, L + 1)\n",
    "        durn_signal = len(signal) / fs\n",
    "        init_scales_fs = (L / init_scales) / durn_signal\n",
    "        if options['use_reduced_lms_scales']:\n",
    "            init_scales_inc_log = init_scales_fs >= plaus_hr_hz[0]\n",
    "        else:\n",
    "            init_scales_inc_log = np.ones_like(init_scales_fs, dtype=bool)  # DIDN\"T FULLY UNDERSTAND\n",
    "\n",
    "        max_scale_index = np.where(init_scales_inc_log)[0]  # DIDN\"T FULLY UNDERSTAND THIS AND NEXT FEW LINES\n",
    "        if max_scale_index.size > 0:\n",
    "            max_scale = max_scale_index[-1] + 1  # Add 1 to convert from 0-based to 1-based index\n",
    "        else:\n",
    "            max_scale = None  # Or handle the case where no scales are valid\n",
    "\n",
    "        # Step 1: calculate local maxima and local minima scalograms\n",
    "\n",
    "        # - detrend\n",
    "        x = scipy.signal.detrend(signal, type=\"linear\")\n",
    "\n",
    "        # - populate LMS matrices\n",
    "        [m_max, m_min] = find_lms_using_msptd_approach(max_scale, x, options)\n",
    "\n",
    "        # Step 2: find the scale with the most local maxima (or local minima)\n",
    "\n",
    "        # - row-wise summation (i.e. sum each row)\n",
    "        if options[\"find_pks\"]:\n",
    "            gamma_max = np.sum(m_max, axis=1)  # the \"axis=1\" option makes it row-wise\n",
    "        if options[\"find_trs\"]:\n",
    "            gamma_min = np.sum(m_min, axis=1)\n",
    "        # - find scale with the most local maxima (or local minima)\n",
    "        if options[\"find_pks\"]:\n",
    "            lambda_max = np.argmax(gamma_max)\n",
    "        if options[\"find_trs\"]:\n",
    "            lambda_min = np.argmax(gamma_min)\n",
    "\n",
    "        # Step 3: Use lambda to remove all elements of m for which k>lambda\n",
    "        first_scale_to_include = np.argmax(init_scales_inc_log)\n",
    "        if options[\"find_pks\"]:\n",
    "            m_max = m_max[first_scale_to_include:lambda_max + 1, :]\n",
    "        if options[\"find_trs\"]:\n",
    "            m_min = m_min[first_scale_to_include:lambda_min + 1, :]\n",
    "\n",
    "        # Step 4: Find peaks (and onsets)\n",
    "        # - column-wise summation\n",
    "        if options[\"find_pks\"]:\n",
    "            m_max_sum = np.sum(m_max == False, axis=0)\n",
    "            peaks = np.where(m_max_sum == 0)[0].astype(int)\n",
    "        else:\n",
    "            peaks = []\n",
    "\n",
    "        if options[\"find_trs\"]:\n",
    "            m_min_sum = np.sum(m_min == False, axis=0)\n",
    "            onsets = np.where(m_min_sum == 0)[0].astype(int)\n",
    "        else:\n",
    "            onsets = []\n",
    "\n",
    "        return peaks, onsets\n",
    "\n",
    "    # ~~~ Main function ~~~\n",
    "\n",
    "    # Specify settings\n",
    "    # - version: optimal selection (CinC 2024)\n",
    "    options = {\n",
    "        'find_trs': True,  # whether or not to find onsets\n",
    "        'find_pks': True,  # whether or not to find peaks\n",
    "        'do_ds': True,  # whether or not to do downsampling\n",
    "        'ds_freq': 20,  # the target downsampling frequency\n",
    "        'use_reduced_lms_scales': True,  # whether or not to reduce the number of scales (default 30 bpm)\n",
    "        'win_len': 8,  # duration of individual windows for analysis\n",
    "        'win_overlap': 0.2,  # proportion of window overlap\n",
    "        'plaus_hr_bpm': [30, 200]  # range of plausible HRs (only the lower bound is used)\n",
    "    }\n",
    "\n",
    "    # Split into overlapping windows\n",
    "    no_samps_in_win = options[\"win_len\"] * sampling_rate\n",
    "    if len(signal) <= no_samps_in_win:\n",
    "        win_starts = 0\n",
    "        win_ends = len(signal) - 1\n",
    "    else:\n",
    "        win_offset = int(round(no_samps_in_win * (1 - options[\"win_overlap\"])))\n",
    "        win_starts = list(range(0, len(signal) - no_samps_in_win + 1, win_offset))\n",
    "        win_ends = [start + 1 + no_samps_in_win for start in win_starts]\n",
    "        if win_ends[-1] < len(signal):\n",
    "            win_starts.append(len(signal) - 1 - no_samps_in_win)\n",
    "            win_ends.append(len(signal))\n",
    "        # this ensures that the windows include the entire signal duration\n",
    "\n",
    "    # Set up downsampling if the sampling frequency is particularly high\n",
    "    if options[\"do_ds\"]:\n",
    "        min_fs = options[\"ds_freq\"]\n",
    "        if sampling_rate > min_fs:\n",
    "            ds_factor = int(np.floor(sampling_rate / min_fs))\n",
    "            ds_fs = sampling_rate / np.floor(sampling_rate / min_fs)\n",
    "        else:\n",
    "            options[\"do_ds\"] = False\n",
    "\n",
    "    # detect peaks and onsets in each window\n",
    "    peaks = []\n",
    "    onsets = []\n",
    "\n",
    "    # cycle through each window\n",
    "    for win_no in range(len(win_starts)):\n",
    "        # Extract this window's data\n",
    "        win_sig = signal[win_starts[win_no]:win_ends[win_no]]\n",
    "\n",
    "        # Downsample signal\n",
    "        if options['do_ds']:\n",
    "            rel_sig = downsample(win_sig, ds_factor)\n",
    "            rel_fs = ds_fs\n",
    "        else:\n",
    "            rel_sig = win_sig\n",
    "            rel_fs = sampling_rate\n",
    "\n",
    "        # Detect peaks and onsets\n",
    "        p, t = detect_peaks_and_onsets_using_msptd(rel_sig, rel_fs, options)\n",
    "\n",
    "        # Resample peaks\n",
    "        if options['do_ds']:\n",
    "            p = [peak * ds_factor for peak in p]\n",
    "            t = [onset * ds_factor for onset in t]\n",
    "\n",
    "        # Correct peak indices by finding highest point within tolerance either side of detected peaks\n",
    "        tol_durn = 0.05\n",
    "        if rel_fs < 10:\n",
    "            tol_durn = 0.2\n",
    "        elif rel_fs < 20:\n",
    "            tol_durn = 0.1\n",
    "        tol = int(np.ceil(rel_fs * tol_durn))\n",
    "\n",
    "        for pk_no in range(len(p)):\n",
    "            segment = win_sig[(p[pk_no] - tol):(p[pk_no] + tol + 1)]\n",
    "            temp = np.argmax(segment)\n",
    "            p[pk_no] = p[pk_no] - tol + temp\n",
    "\n",
    "        # Correct onset indices by finding highest point within tolerance either side of detected onsets\n",
    "        for onset_no in range(len(t)):\n",
    "            segment = win_sig[(t[onset_no] - tol):(t[onset_no] + tol + 1)]\n",
    "            temp = np.argmin(segment)\n",
    "            t[onset_no] = t[onset_no] - tol + temp\n",
    "\n",
    "        # Store peaks and onsets\n",
    "        win_peaks = [peak + win_starts[win_no] for peak in p]\n",
    "        peaks.extend(win_peaks)\n",
    "        win_onsets = [onset + win_starts[win_no] for onset in t]\n",
    "        onsets.extend(win_onsets)\n",
    "\n",
    "    # Tidy up detected peaks and onsets (by ordering them and only retaining unique ones)\n",
    "    peaks = sorted(set(peaks))\n",
    "    onsets = sorted(set(onsets))\n",
    "\n",
    "    # Plot results (optional)\n",
    "    if show:\n",
    "        _, ax0 = plt.subplots(nrows=1, ncols=1, sharex=True)\n",
    "        ax0.plot(signal, label=\"signal\")\n",
    "        ax0.scatter(peaks, signal[peaks], c=\"r\")\n",
    "        ax0.scatter(onsets, signal[onsets], c=\"b\")\n",
    "        ax0.set_title(\"PPG Onsets (Method by Charlton et al., 2024)\")\n",
    "\n",
    "    return peaks, onsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heart_rate.ppg_utils import signal_fixpeaks\n",
    "from heart_rate.hrv.heart_rate_fragmentation import compute_HRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RMSSD** is calculated on 5-minute windows as extensively done in the literature (e.g., https://pmc.ncbi.nlm.nih.gov/articles/PMC10566244/). Usually, consecutive windows are used, but since my computation is quite fast I am now opting for an overlap of 4 minutes (step of 1 min) between windows to increase the granularity. Moreover, in this way, I do not lose the last part of my quite portion (I loose at max 1 min). This choice can be discussed (PS I tried with 1s step and results seem interesing....)\n",
    "\n",
    "**SDNN** I don't remember what Silvani said it's better to do (I think 5 min windows also for this)\n",
    "\n",
    "For **heart rate fragmentation**, the longer the window the better (https://physionet.org/content/heart-rate-fragmentation-code/1.0.0/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_bursts = 35\n",
    "window_length = pd.Timedelta(\"5 min\")  # window length\n",
    "window_step = pd.Timedelta(\"1 min\")  # window step\n",
    "\n",
    "HRV = []  # Reset HRV storage\n",
    "\n",
    "ibi_quiet_all = []\n",
    "\n",
    "for i, (start_sleep, end_sleep) in enumerate(start_end_sleep):  # for each night\n",
    "\n",
    "    acc_night = compute_acc_SMV(acc_df.loc[start_sleep:end_sleep])\n",
    "    ppg_night = ppg_df.loc[start_sleep:end_sleep]\n",
    "\n",
    "    # Detect wrist accelerometer bursts\n",
    "    bursts = detect_bursts(acc_night, sampling_rate=64, alfa=threshold_bursts/1000)\n",
    "\n",
    "    # Extract quiet periods (no movement of the wrist)\n",
    "    quiet_periods = pd.DataFrame()\n",
    "    quiet_periods[\"start\"] = bursts[\"end\"].iloc[:-1].reset_index(drop=True)\n",
    "    quiet_periods[\"end\"] = bursts[\"start\"].iloc[1:].reset_index(drop=True)\n",
    "\n",
    "    for _, quiet_period in quiet_periods.iterrows():  # for each quiet period\n",
    "\n",
    "        duration_quiet_period = quiet_period[\"end\"] - quiet_period[\"start\"]\n",
    "\n",
    "        if duration_quiet_period < window_length:  # If the whole period is shorter than 5 min, skip it\n",
    "            continue\n",
    "            \n",
    "        acc_quiet = acc_night.loc[quiet_period[\"start\"]:quiet_period[\"end\"]]\n",
    "        ppg_quiet = ppg_night.loc[quiet_period[\"start\"]:quiet_period[\"end\"]]\n",
    "         # Extract systolic peaks from the quiet PPG signal\n",
    "        _, peaks = ppg_findpeaks_charlton(ppg_quiet[\"ppg\"].values, sampling_rate=64)\n",
    "        t_peaks = ppg_quiet.index.to_series().values[peaks]\n",
    "        ibi = np.diff(t_peaks).astype('timedelta64[ns]').astype('float64') / 1e9  # seconds\n",
    "        ibi = np.insert(ibi, 0, np.mean(ibi[1:10]), axis=0)  # Set first value as mean of next 10\n",
    "        ibi = pd.Series(ibi, index=t_peaks)\n",
    "\n",
    "        # Kubios artifact correction\n",
    "        artifacts, env_diff_corrected = signal_fixpeaks(ibi.values, 64, iterative=False)\n",
    "        artifacts_all = np.concatenate((artifacts[\"ectopic\"], artifacts[\"missed\"], artifacts[\"extra\"], artifacts[\"longshort\"]))\n",
    "        ibi[ibi.index[artifacts_all.astype(int)]] = np.nan\n",
    "        ibi_clean = ibi.interpolate(method=\"linear\")\n",
    "\n",
    "        # Generate overlapping windows of 5 minutes with 30-second overlap\n",
    "        current_start = quiet_period[\"start\"]\n",
    "        \n",
    "        while current_start + window_length <= quiet_period[\"end\"]:\n",
    "\n",
    "            current_end = current_start + window_length\n",
    "\n",
    "            ibi_window = ibi_clean.loc[current_start:current_end]\n",
    "\n",
    "            # HRV Features\n",
    "            ppi = ibi_window.values * 1000  # Convert to ms\n",
    "            diff_ppi = np.diff(ppi)\n",
    "\n",
    "            rmssd = np.sqrt(np.mean(diff_ppi**2))  # RMSSD\n",
    "            sdnn = np.std(ppi, ddof=1)  # SDNN\n",
    "            PIP = compute_HRF(ppi)  # Custom HRF computation\n",
    "\n",
    "            HRV.append({\n",
    "                \"day\": i+1,\n",
    "                \"time\": current_start + window_length / 2,\n",
    "                \"rmssd\": rmssd, \n",
    "                \"sdnn\": sdnn, \n",
    "                \"PIP\": PIP\n",
    "            })\n",
    "\n",
    "            current_start += window_step  # Move to next overlapping window\n",
    "\n",
    "        ibi_quiet_all.append(ibi_clean)\n",
    "\n",
    "HRV_df = pd.DataFrame(HRV)\n",
    "ibi_quiet_df = pd.concat(ibi_quiet_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>rmssd</th>\n",
       "      <th>sdnn</th>\n",
       "      <th>PIP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-05-21 01:15:30.448210001</td>\n",
       "      <td>51.492939</td>\n",
       "      <td>70.134558</td>\n",
       "      <td>0.682836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-05-21 01:16:30.448210001</td>\n",
       "      <td>63.893382</td>\n",
       "      <td>72.891992</td>\n",
       "      <td>0.684615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-05-21 01:17:30.448210001</td>\n",
       "      <td>71.148707</td>\n",
       "      <td>77.681860</td>\n",
       "      <td>0.685039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-05-21 01:18:30.448210001</td>\n",
       "      <td>75.701487</td>\n",
       "      <td>79.603955</td>\n",
       "      <td>0.677419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-05-21 01:19:30.448210001</td>\n",
       "      <td>73.139617</td>\n",
       "      <td>79.607552</td>\n",
       "      <td>0.679012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   day                          time      rmssd       sdnn       PIP\n",
       "0    1 2024-05-21 01:15:30.448210001  51.492939  70.134558  0.682836\n",
       "1    1 2024-05-21 01:16:30.448210001  63.893382  72.891992  0.684615\n",
       "2    1 2024-05-21 01:17:30.448210001  71.148707  77.681860  0.685039\n",
       "3    1 2024-05-21 01:18:30.448210001  75.701487  79.603955  0.677419\n",
       "4    1 2024-05-21 01:19:30.448210001  73.139617  79.607552  0.679012"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HRV_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x142eca9c0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 18:32:46.520 python[40896:1407279] +[IMKClient subclass]: chose IMKClient_Modern\n"
     ]
    }
   ],
   "source": [
    "HRV_df = pd.DataFrame(HRV)\n",
    "\n",
    "plt.figure(figsize=(19, 11))\n",
    "plt.subplot(2,1,1)\n",
    "plt.scatter(HRV_df[\"time\"], HRV_df[\"rmssd\"], label=\"RMSSD\")\n",
    "plt.subplot(2,1,2, sharex=plt.subplot(2,1,1))\n",
    "plt.scatter(ibi_quiet_df.index, ibi_quiet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"fdc41dec-b11b-411d-a327-473149cfb609\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"fdc41dec-b11b-411d-a327-473149cfb609\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.6.2.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"fdc41dec-b11b-411d-a327-473149cfb609\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'time'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/DARE/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'time'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m HRV_df_plot \u001b[38;5;241m=\u001b[39m HRV_df\u001b[38;5;241m.\u001b[39mresample(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1min\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     23\u001b[0m p2 \u001b[38;5;241m=\u001b[39m figure(x_axis_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m\"\u001b[39m, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmssd\u001b[39m\u001b[38;5;124m\"\u001b[39m, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1100\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, x_range\u001b[38;5;241m=\u001b[39mp1\u001b[38;5;241m.\u001b[39mx_range)\n\u001b[0;32m---> 24\u001b[0m p2\u001b[38;5;241m.\u001b[39mline(\u001b[43mHRV_df_plot\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, HRV_df_plot[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmssd\u001b[39m\u001b[38;5;124m\"\u001b[39m], legend_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmssd\u001b[39m\u001b[38;5;124m\"\u001b[39m, line_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m p2\u001b[38;5;241m.\u001b[39mscatter(HRV_df_plot[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m], HRV_df_plot[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmssd\u001b[39m\u001b[38;5;124m\"\u001b[39m], legend_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmssd\u001b[39m\u001b[38;5;124m\"\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m\"\u001b[39m, size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m11\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Horizontal line at the median, dashed\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DARE/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DARE/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'time'"
     ]
    }
   ],
   "source": [
    "output_notebook()\n",
    "\n",
    "p1 = figure(x_axis_type=\"datetime\", title=\"IBI\", width=1100, height=300)\n",
    "p1.line(ibi_quiet_df.index, ibi_quiet_df.values, line_width=2)\n",
    "p1.title.text_font_size = \"16pt\"\n",
    "p1.yaxis.axis_label = \"IBI (s)\"\n",
    "p1.xaxis.axis_label_text_font_size = \"16pt\"\n",
    "p1.yaxis.axis_label_text_font_size = \"16pt\"\n",
    "p1.xaxis.major_label_text_font_size = \"16pt\"\n",
    "p1.yaxis.major_label_text_font_size = \"16pt\"\n",
    "p1.xaxis.formatter=DatetimeTickFormatter(\n",
    "        hours=\"%H:%M\",\n",
    "        days=\"%d %B\",\n",
    "        months=\"%d %B\",\n",
    "        years=\"%d %B\",\n",
    "    )\n",
    "p1.xaxis.major_label_orientation = np.pi/4\n",
    "\n",
    "HRV_df.index = pd.to_datetime(HRV_df[\"time\"])\n",
    "HRV_df.drop(columns=[\"time\"], inplace=True)\n",
    "HRV_df_plot = HRV_df.resample(\"1min\").mean()\n",
    "\n",
    "p2 = figure(x_axis_type=\"datetime\", title=\"rmssd\", width=1100, height=300, x_range=p1.x_range)\n",
    "p2.line(HRV_df_plot[\"time\"], HRV_df_plot[\"rmssd\"], legend_label=\"rmssd\", line_width=2, color=\"blue\")\n",
    "p2.scatter(HRV_df_plot[\"time\"], HRV_df_plot[\"rmssd\"], legend_label=\"rmssd\", color=\"blue\", size = 11)\n",
    "# Horizontal line at the median, dashed\n",
    "p2.line([HRV_df_plot[\"time\"].iloc[0], HRV_df_plot[\"time\"].iloc[-1]], [np.median(HRV_df_plot[\"rmssd\"]), np.median(HRV_df_plot[\"rmssd\"])], \n",
    "        line_width=2, color=\"black\", line_dash=\"dashed\", legend_label=\"Median\")\n",
    "p2.title.text_font_size = \"16pt\"\n",
    "p2.legend.location = \"top_left\"\n",
    "p2.legend.click_policy=\"hide\"\n",
    "p2.yaxis.axis_label = \"rmssd (ms)\"\n",
    "p2.xaxis.axis_label_text_font_size = \"16pt\"\n",
    "p2.yaxis.axis_label_text_font_size = \"16pt\"\n",
    "p2.xaxis.major_label_text_font_size = \"16pt\"\n",
    "p2.yaxis.major_label_text_font_size = \"16pt\"\n",
    "# p2.y_range = Range1d(20, 110)\n",
    "p2.xaxis.formatter=DatetimeTickFormatter(\n",
    "        hours=\"%H:%M\",\n",
    "        days=\"%d %B\",\n",
    "        months=\"%d %B\",\n",
    "        years=\"%d %B\",\n",
    "    )\n",
    "p2.xaxis.major_label_orientation = np.pi/4\n",
    "\n",
    "p3 = figure(x_axis_type=\"datetime\", title=\"HR fragmentation\", width=1100, height=300, x_range=p1.x_range)\n",
    "p3.line(HRV_df_plot[\"time\"], HRV_df_plot[\"PIP\"], legend_label=\"PIP\", line_width=2, color=\"blue\")\n",
    "p3.scatter(HRV_df_plot[\"time\"], HRV_df_plot[\"PIP\"], legend_label=\"PIP\", color=\"blue\", size = 11)\n",
    "# Horizontal line at the median, dashed\n",
    "p3.line([HRV_df_plot[\"time\"].iloc[0], HRV_df_plot[\"time\"].iloc[-1]], [np.median(HRV_df_plot[\"PIP\"]), np.median(HRV_df_plot[\"PIP\"])], \n",
    "        line_width=2, color=\"black\", line_dash=\"dashed\", legend_label=\"Median\")\n",
    "p3.title.text_font_size = \"16pt\"\n",
    "p3.legend.location = \"top_left\"\n",
    "p3.legend.click_policy=\"hide\"\n",
    "p3.xaxis.axis_label = \"Time\"\n",
    "p3.yaxis.axis_label = \"PIP (%)\"\n",
    "p3.xaxis.axis_label_text_font_size = \"16pt\"\n",
    "p3.yaxis.axis_label_text_font_size = \"16pt\"\n",
    "p3.xaxis.major_label_text_font_size = \"16pt\"\n",
    "p3.yaxis.major_label_text_font_size = \"16pt\"\n",
    "p3.xaxis.formatter=DatetimeTickFormatter(\n",
    "        hours=\"%H:%M\",\n",
    "        days=\"%d %B\",\n",
    "        months=\"%d %B\",\n",
    "        years=\"%d %B\",\n",
    "    )\n",
    "p3.xaxis.major_label_orientation = np.pi/4\n",
    "\n",
    "# Show the plot\n",
    "show(gridplot([[p1], [p2], [p3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>window_start</th>\n",
       "      <th>window_end</th>\n",
       "      <th>rmssd</th>\n",
       "      <th>sdnn</th>\n",
       "      <th>PIP</th>\n",
       "      <th>middle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-05-21 01:13:01.088835001</td>\n",
       "      <td>2024-05-21 01:18:01.088835001</td>\n",
       "      <td>51.686506</td>\n",
       "      <td>64.647549</td>\n",
       "      <td>0.684825</td>\n",
       "      <td>2024-05-21 01:15:31.088835001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-05-21 01:13:31.088835001</td>\n",
       "      <td>2024-05-21 01:18:31.088835001</td>\n",
       "      <td>56.308176</td>\n",
       "      <td>62.170712</td>\n",
       "      <td>0.683794</td>\n",
       "      <td>2024-05-21 01:16:01.088835001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-05-21 01:14:01.088835001</td>\n",
       "      <td>2024-05-21 01:19:01.088835001</td>\n",
       "      <td>65.587052</td>\n",
       "      <td>74.812008</td>\n",
       "      <td>0.681275</td>\n",
       "      <td>2024-05-21 01:16:31.088835001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-05-21 01:14:31.088835001</td>\n",
       "      <td>2024-05-21 01:19:31.088835001</td>\n",
       "      <td>68.600474</td>\n",
       "      <td>79.210685</td>\n",
       "      <td>0.677419</td>\n",
       "      <td>2024-05-21 01:17:01.088835001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-05-21 01:15:01.088835001</td>\n",
       "      <td>2024-05-21 01:20:01.088835001</td>\n",
       "      <td>71.855343</td>\n",
       "      <td>77.171793</td>\n",
       "      <td>0.680328</td>\n",
       "      <td>2024-05-21 01:17:31.088835001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2368</th>\n",
       "      <td>6</td>\n",
       "      <td>2024-05-27 05:43:28.464230061</td>\n",
       "      <td>2024-05-27 05:48:28.464230061</td>\n",
       "      <td>30.272144</td>\n",
       "      <td>46.502956</td>\n",
       "      <td>0.663082</td>\n",
       "      <td>2024-05-27 05:45:58.464230061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2369</th>\n",
       "      <td>6</td>\n",
       "      <td>2024-05-27 05:43:58.464230061</td>\n",
       "      <td>2024-05-27 05:48:58.464230061</td>\n",
       "      <td>29.064951</td>\n",
       "      <td>48.197264</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2024-05-27 05:46:28.464230061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2370</th>\n",
       "      <td>6</td>\n",
       "      <td>2024-05-27 05:44:28.464230061</td>\n",
       "      <td>2024-05-27 05:49:28.464230061</td>\n",
       "      <td>27.299414</td>\n",
       "      <td>53.358816</td>\n",
       "      <td>0.699301</td>\n",
       "      <td>2024-05-27 05:46:58.464230061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>6</td>\n",
       "      <td>2024-05-27 05:49:44.401791096</td>\n",
       "      <td>2024-05-27 05:54:44.401791096</td>\n",
       "      <td>42.737534</td>\n",
       "      <td>52.782592</td>\n",
       "      <td>0.621528</td>\n",
       "      <td>2024-05-27 05:52:14.401791096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372</th>\n",
       "      <td>6</td>\n",
       "      <td>2024-05-27 05:54:58.808041096</td>\n",
       "      <td>2024-05-27 05:59:58.808041096</td>\n",
       "      <td>36.875507</td>\n",
       "      <td>58.651174</td>\n",
       "      <td>0.686833</td>\n",
       "      <td>2024-05-27 05:57:28.808041096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2373 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      day                  window_start                    window_end  \\\n",
       "0       1 2024-05-21 01:13:01.088835001 2024-05-21 01:18:01.088835001   \n",
       "1       1 2024-05-21 01:13:31.088835001 2024-05-21 01:18:31.088835001   \n",
       "2       1 2024-05-21 01:14:01.088835001 2024-05-21 01:19:01.088835001   \n",
       "3       1 2024-05-21 01:14:31.088835001 2024-05-21 01:19:31.088835001   \n",
       "4       1 2024-05-21 01:15:01.088835001 2024-05-21 01:20:01.088835001   \n",
       "...   ...                           ...                           ...   \n",
       "2368    6 2024-05-27 05:43:28.464230061 2024-05-27 05:48:28.464230061   \n",
       "2369    6 2024-05-27 05:43:58.464230061 2024-05-27 05:48:58.464230061   \n",
       "2370    6 2024-05-27 05:44:28.464230061 2024-05-27 05:49:28.464230061   \n",
       "2371    6 2024-05-27 05:49:44.401791096 2024-05-27 05:54:44.401791096   \n",
       "2372    6 2024-05-27 05:54:58.808041096 2024-05-27 05:59:58.808041096   \n",
       "\n",
       "          rmssd       sdnn       PIP                        middle  \n",
       "0     51.686506  64.647549  0.684825 2024-05-21 01:15:31.088835001  \n",
       "1     56.308176  62.170712  0.683794 2024-05-21 01:16:01.088835001  \n",
       "2     65.587052  74.812008  0.681275 2024-05-21 01:16:31.088835001  \n",
       "3     68.600474  79.210685  0.677419 2024-05-21 01:17:01.088835001  \n",
       "4     71.855343  77.171793  0.680328 2024-05-21 01:17:31.088835001  \n",
       "...         ...        ...       ...                           ...  \n",
       "2368  30.272144  46.502956  0.663082 2024-05-27 05:45:58.464230061  \n",
       "2369  29.064951  48.197264  0.666667 2024-05-27 05:46:28.464230061  \n",
       "2370  27.299414  53.358816  0.699301 2024-05-27 05:46:58.464230061  \n",
       "2371  42.737534  52.782592  0.621528 2024-05-27 05:52:14.401791096  \n",
       "2372  36.875507  58.651174  0.686833 2024-05-27 05:57:28.808041096  \n",
       "\n",
       "[2373 rows x 7 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HRV_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x451162b70>]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(19, 11))\n",
    "\n",
    "days = HRV_df[\"day\"].unique()\n",
    "\n",
    "plt.figure(figsize=(19, 11))\n",
    "plt.subplot(2, 1, 1)\n",
    "for day in days:\n",
    "    HRV_day = HRV_df[HRV_df[\"day\"] == day]\n",
    "    plt.plot(HRV_day[\"middle\"], HRV_day[\"rmssd\"], '-o', label=f\"day {day}\")\n",
    "\n",
    "plt.legend()\n",
    "plt.subplot(2, 1, 2, sharex = plt.subplot(2, 1, 1))\n",
    "plt.plot(ibi_quiet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2024-05-20 01:02:45')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime(str(acc_df.index[0].date()) + \" \" + output_GGIR[\"sleeponset_ts\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_GGIR_path = \"/Users/augenpro/Documents/Empatica/data_sara/data/GGIR_output/\"\n",
    "\n",
    "output_GGIR = pd.read_csv(output_GGIR_path + \"part4_nightsummary_sleep_full.csv\")\n",
    "\n",
    "tst = output_GGIR[\"SleepDurationInSpt\"].values[0]\n",
    "\n",
    "waso = output_GGIR[\"WASO\"].values[0]\n",
    "\n",
    "SE = output_GGIR[\"SE\"]\n",
    "\n",
    "SE = tst / (tst + waso)\n",
    "\n",
    "# Sleep and wake onsets\n",
    "if output_GGIR[\"sleeponset_ts\"].iloc[0][0] == '0':\n",
    "    sleep_onset = pd.to_datetime(str(acc_norm.index[0].date() + pd.Timedelta(\"1d\")) + \" \" + output_GGIR[\"sleeponset_ts\"].iloc[0])\n",
    "else: \n",
    "    sleep_onset = pd.to_datetime(str(acc_norm.index[0].date())  + \" \" + output_GGIR[\"sleeponset_ts\"].iloc[0])\n",
    "\n",
    "wake_onset = pd.to_datetime(str(acc_norm.index[0].date() + pd.Timedelta(\"1d\")) + \" \" + output_GGIR[\"wakeup_ts\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-05-20 13:02:55.531980990</th>\n",
       "      <td>-0.068359</td>\n",
       "      <td>0.408691</td>\n",
       "      <td>0.931152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-20 13:02:55.547605991</th>\n",
       "      <td>-0.068359</td>\n",
       "      <td>0.403320</td>\n",
       "      <td>0.930176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-20 13:02:55.563230991</th>\n",
       "      <td>-0.071289</td>\n",
       "      <td>0.397949</td>\n",
       "      <td>0.919434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-20 13:02:55.578855991</th>\n",
       "      <td>-0.069824</td>\n",
       "      <td>0.400879</td>\n",
       "      <td>0.925781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-20 13:02:55.594480991</th>\n",
       "      <td>-0.064453</td>\n",
       "      <td>0.409668</td>\n",
       "      <td>0.923340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      x         y         z\n",
       "2024-05-20 13:02:55.531980990 -0.068359  0.408691  0.931152\n",
       "2024-05-20 13:02:55.547605991 -0.068359  0.403320  0.930176\n",
       "2024-05-20 13:02:55.563230991 -0.071289  0.397949  0.919434\n",
       "2024-05-20 13:02:55.578855991 -0.069824  0.400879  0.925781\n",
       "2024-05-20 13:02:55.594480991 -0.064453  0.409668  0.923340"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc.index.is_monotonic_increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DARE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
