{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading AVRO files and non-wear detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['lines.linewidth'] = 0.91\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# % matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data\n",
    "~ 2 min for reading and concatenating into a list for 7 days of data\n",
    "\n",
    "**With Pandas**\n",
    "\n",
    "- ~ 2.30 min for transforming it into a pd.DataFrame\n",
    "- ~ 1.30 min for saving to accelerometer to csv\n",
    "- ~ 3 sec for saving ppg, acc, and temp to parquet\n",
    "\n",
    "**With Polars**\n",
    "\n",
    "- TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_io.embraceplus.read_avro import ReadEmpaticaAvro\n",
    "empatica_reader = ReadEmpaticaAvro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Change the paths below to the location of the data on your machine ####\n",
    "data_path = \"/Users/augenpro/Documents/Empatica/data_sara/data/participant_data/\"\n",
    "\n",
    "#### Change the subject ID and device ID below to the subject and device you want to process ####\n",
    "sub_ID = \"00007\"\n",
    "device_ID = \"3YK3J151VJ\"\n",
    "\n",
    "days = sorted(os.listdir(data_path))\n",
    "days = [day for day in days if day[0] != \".\"] # remove hidden files (needed for MacOS users)\n",
    "\n",
    "acc = []\n",
    "ppg = []\n",
    "temp = []\n",
    "time = []\n",
    "time_temp = []\n",
    "\n",
    "for i, day in enumerate(days):\n",
    "    \n",
    "    print(f\"Processing day {i+1}/{len(days)}\")\n",
    "\n",
    "    folder_day = data_path + day + \"/\" + sub_ID + \"-\" + device_ID + \"/raw_data/v6\"\n",
    "\n",
    "    avro_files = sorted(glob.glob(folder_day + \"/*.avro\"))\n",
    "\n",
    "    for avro_file in avro_files:\n",
    "        \n",
    "        data = empatica_reader.read(file=avro_file)\n",
    "\n",
    "        acc.extend(data[\"acc\"])\n",
    "\n",
    "        ppg.extend(data[\"bvp\"])\n",
    "\n",
    "        time.extend(data[\"time\"]) # Same for acc and ppg\n",
    "\n",
    "        temp.extend(data[\"temp\"])\n",
    "        time_temp.extend(data[\"time_temp\"])\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df = pd.DataFrame(acc, columns=[\"x\", \"y\", \"z\"], index = pd.to_datetime(time, unit=\"s\")).sort_index()\n",
    "ppg_df = pd.DataFrame(ppg, columns=[\"ppg\"], index = pd.to_datetime(time, unit=\"s\")).sort_index()\n",
    "temp_df = pd.DataFrame(temp, columns=[\"temp\"], index = pd.to_datetime(time_temp, unit=\"s\")).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect non-wear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to divide it into portions when the device was in charge\n",
    "\n",
    "t_charge_end = acc_df.index[acc_df.index.to_series().diff().dt.total_seconds() > 60*10] # if the difference between two consecutive timestamps is more than 10 minutes (**), it means the device was in charge\n",
    "t_charge_start = acc_df.index[np.where(acc_df.index.to_series().diff().dt.total_seconds() > 60*10)[0]-1] # the start of the charge is the timestamp before the end of the charge\n",
    "t_charge = pd.DataFrame({\"start\": t_charge_start, \"end\": t_charge_end})\n",
    "\n",
    "good_portions = pd.DataFrame(columns=[\"start\", \"end\"]) # I will store the portions when the device was not in charge here\n",
    "good_portions[\"start\"] = t_charge[\"end\"].iloc[:-1].reset_index(drop=True)\n",
    "good_portions[\"end\"] = t_charge[\"start\"].iloc[1:].reset_index(drop=True)\n",
    "start_first_charge = t_charge[\"start\"].iloc[0]\n",
    "end_last_charge = t_charge[\"end\"].iloc[-1]\n",
    "\n",
    "# Segment the data into portions when the device was not in charge and perform nonwear detection\n",
    "# Add the first portion\n",
    "acc_df_portions = [acc_df[:start_first_charge]]\n",
    "ppg_df_portions = [ppg_df[:start_first_charge]]\n",
    "temp_df_portions = [temp_df[:start_first_charge]]\n",
    "\n",
    "for i, row in good_portions.iterrows():\n",
    "\n",
    "    if row[\"end\"] - row[\"start\"] < pd.Timedelta(\"10 min\"): # if the portion is less than 10 minutes (**), skip it\n",
    "        continue\n",
    "\n",
    "    acc_df_portions.append(acc_df[row[\"start\"]:row[\"end\"]])\n",
    "    ppg_df_portions.append(ppg_df[row[\"start\"]:row[\"end\"]])\n",
    "    temp_df_portions.append(temp_df[row[\"start\"]:row[\"end\"]])\n",
    "\n",
    "# Add the last portion\n",
    "acc_df_portions.append(acc_df[end_last_charge:])\n",
    "ppg_df_portions.append(ppg_df[end_last_charge:])\n",
    "temp_df_portions.append(temp_df[end_last_charge:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nonwear.DETACH import nimbaldetach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each of them, perform NW detection\n",
    "acc_df_cleaned = []\n",
    "temp_df_cleaned = []\n",
    "ppg_df_cleaned = []\n",
    "for i, (acc, temp, ppg) in enumerate(zip(acc_df_portions, temp_df_portions, ppg_df_portions)):\n",
    "\n",
    "    start_stop_nw, _ = nimbaldetach(acc['x'].values, acc['y'].values, acc['z'].values, temp[\"temp\"].values, accel_freq=64, temperature_freq=1, quiet=True)\n",
    "\n",
    "    # Remove non-wear periods\n",
    "    for i, row in start_stop_nw.iterrows():\n",
    "        datetime_start_nw = acc.index[row[\"Start Datapoint\"]]\n",
    "        datetime_end_nw = acc.index[row[\"End Datapoint\"]]\n",
    "        acc.loc[datetime_start_nw:datetime_end_nw] = np.nan\n",
    "        temp.loc[datetime_start_nw:datetime_end_nw] = np.nan\n",
    "        ppg.loc[datetime_start_nw:datetime_end_nw] = np.nan\n",
    "    acc_portion = acc.dropna()\n",
    "    temp_portion = temp.dropna()\n",
    "    ppg_portion = ppg.dropna()\n",
    "\n",
    "    acc_df_cleaned.append(acc_portion)\n",
    "    temp_df_cleaned.append(temp_portion)\n",
    "    ppg_df_cleaned.append(ppg_portion)\n",
    "\n",
    "acc_df_cleaned = pd.concat(acc_df_cleaned)\n",
    "temp_df_cleaned = pd.concat(temp_df_cleaned)\n",
    "ppg_df_cleaned = pd.concat(ppg_df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data_path = \"xxx\"\n",
    "\n",
    "# Save to csv for GGIR\n",
    "# acc_df.to_csv(save_data_path + \"/acc_new.csv\") \n",
    "\n",
    "# Save to parquet for further analysis\n",
    "acc_df_cleaned.to_parquet(save_data_path + \"/acc.parquet\")\n",
    "temp_df_cleaned.to_parquet(save_data_path + \"/temp.parquet\")\n",
    "ppg_df_cleaned.to_parquet(save_data_path + \"/ppg.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization.check_accelerometer_temp import plot_acc_temp\n",
    "\n",
    "# Decide whether to plot in notebook or in a separate window\n",
    "from bokeh.plotting import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "processed_data_path = \"/Users/augenpro/Documents/Empatica/data_sara/data/GGIR_input/\"\n",
    "plot_acc_temp(processed_data_path, accSMV = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GGIR output - work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "start",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "end",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "ac3fda86-d738-4c0d-8bd4-a4b21cc8016a",
       "rows": [
        [
         "0",
         "2024-05-21 01:02:45",
         "2024-05-21 08:07:30"
        ],
        [
         "1",
         "2024-05-22 01:03:15",
         "2024-05-22 05:59:55"
        ],
        [
         "2",
         "2024-05-23 02:37:25",
         "2024-05-23 06:10:10"
        ],
        [
         "3",
         "2024-05-24 00:22:25",
         "2024-05-24 07:41:30"
        ],
        [
         "4",
         "2024-05-25 01:03:10",
         "2024-05-25 06:07:55"
        ],
        [
         "5",
         "2024-05-26 23:48:50",
         "2024-05-26 09:49:05"
        ],
        [
         "6",
         "2024-05-27 23:39:40",
         "2024-05-27 06:14:20"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 7
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-21 01:02:45</td>\n",
       "      <td>2024-05-21 08:07:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-05-22 01:03:15</td>\n",
       "      <td>2024-05-22 05:59:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-23 02:37:25</td>\n",
       "      <td>2024-05-23 06:10:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-05-24 00:22:25</td>\n",
       "      <td>2024-05-24 07:41:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-05-25 01:03:10</td>\n",
       "      <td>2024-05-25 06:07:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-05-26 23:48:50</td>\n",
       "      <td>2024-05-26 09:49:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-05-27 23:39:40</td>\n",
       "      <td>2024-05-27 06:14:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                start                 end\n",
       "0 2024-05-21 01:02:45 2024-05-21 08:07:30\n",
       "1 2024-05-22 01:03:15 2024-05-22 05:59:55\n",
       "2 2024-05-23 02:37:25 2024-05-23 06:10:10\n",
       "3 2024-05-24 00:22:25 2024-05-24 07:41:30\n",
       "4 2024-05-25 01:03:10 2024-05-25 06:07:55\n",
       "5 2024-05-26 23:48:50 2024-05-26 09:49:05\n",
       "6 2024-05-27 23:39:40 2024-05-27 06:14:20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_GGIR_path = \"/Users/augenpro/Documents/Empatica/data_sara/data/GGIR_output/output_GGIR_input/results/QC/\"\n",
    "\n",
    "output_GGIR = pd.read_csv(output_GGIR_path + \"part4_nightsummary_sleep_full.csv\")\n",
    "\n",
    "SPT = []\n",
    "\n",
    "for i, day_row in output_GGIR.iterrows():\n",
    "    # Stupid thing to get the correct datetime for segmenting signals into day and night (but no alternatives I guess)\n",
    "    if output_GGIR[\"sleeponset_ts\"].iloc[0][0] == '0':\n",
    "        sleep_onset = pd.to_datetime(str(pd.to_datetime(day_row[\"calendar_date\"]).date() + pd.Timedelta(\"1d\")) + \" \" + day_row[\"sleeponset_ts\"])\n",
    "    else:\n",
    "        sleep_onset = pd.to_datetime(pd.to_datetime(day_row[\"calendar_date\"]).date() + \" \" + day_row[\"sleeponset_ts\"])\n",
    "\n",
    "    wake_onset = pd.to_datetime(str(pd.to_datetime(day_row[\"calendar_date\"]).date() + pd.Timedelta(\"1d\")) + \" \" + day_row[\"wakeup_ts\"])\n",
    "\n",
    "    SPT.append((sleep_onset, wake_onset))\n",
    "\n",
    "start_end_sleep = np.array(SPT).reshape(-1, 2)\n",
    "SPT_GGIR = pd.DataFrame(start_end_sleep, columns=[\"start\", \"end\"])\n",
    "SPT_GGIR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DARE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
