{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['lines.linewidth'] = 0.91\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import cheby1, sosfiltfilt\n",
    "\n",
    "def apply_resample(\n",
    "    *, time, goal_fs=None, time_rs=None, data=(), indices=(), aa_filter=True, fs=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply a resample to a set of data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    time : numpy.ndarray\n",
    "        Array of original timestamps.\n",
    "    goal_fs : float, optional\n",
    "        Desired sampling frequency in Hz.  One of `goal_fs` or `time_rs` must be\n",
    "        provided.\n",
    "    time_rs : numpy.ndarray, optional\n",
    "        Resampled time series to sample to. One of `goal_fs` or `time_rs` must be\n",
    "        provided.\n",
    "    data : tuple, optional\n",
    "        Tuple of arrays to normally downsample using np.interpolation. Must match the\n",
    "        size of `time`. Can handle `None` inputs, and will return an array of zeros\n",
    "        matching the downsampled size.\n",
    "    indices : tuple, optional\n",
    "        Tuple of arrays of indices to downsample.\n",
    "    aa_filter : bool, optional\n",
    "        Apply an anti-aliasing filter before downsampling. Default is True. This\n",
    "        is the same filter as used by :py:function:`scipy.signal.decimate`.\n",
    "        See [1]_ for details. Ignored if upsampling.\n",
    "    fs : {None, float}, optional\n",
    "        Original sampling frequency in Hz. If `goal_fs` is an integer factor\n",
    "        of `fs`, every nth sample will be taken, otherwise `np.np.interp` will be\n",
    "        used. Leave blank to always use `np.np.interp`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    time_rs : numpy.ndarray\n",
    "        Resampled time.\n",
    "    data_rs : tuple, optional\n",
    "        Resampled data, if provided.\n",
    "    indices_rs : tuple, optional\n",
    "        Resampled indices, if provided.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] https://en.wikipedia.org/wiki/Downsampling_(signal_processing)\n",
    "    \"\"\"\n",
    "\n",
    "    def resample(x, factor, t, t_rs):\n",
    "        if (int(factor) == factor) and (factor > 1):\n",
    "            # in case that t_rs is provided and ends earlier than t\n",
    "            n = np.nonzero(t <= t_rs[-1])[0][-1] + 1\n",
    "            return (x[: n : int(factor)],)\n",
    "        else:\n",
    "            if x.ndim == 1:\n",
    "                return (np.interp(t_rs, t, x),)\n",
    "            elif x.ndim == 2:\n",
    "                xrs = np.zeros((t_rs.size, x.shape[1]), dtype=np.float64)\n",
    "                for j in range(x.shape[1]):\n",
    "                    xrs[:, j] = np.interp(t_rs, t, x[:, j])\n",
    "                return (xrs,)\n",
    "\n",
    "    if fs is None:\n",
    "        # compute sampling frequency by hand\n",
    "        fs = 1 / np.mean(np.diff(time[:5000]))\n",
    "\n",
    "    if time_rs is None and goal_fs is None:\n",
    "        raise ValueError(\"One of `time_rs` or `goal_fs` is required.\")\n",
    "\n",
    "    # get resampled time if necessary\n",
    "    if time_rs is None:\n",
    "        if int(fs / goal_fs) == fs / goal_fs and goal_fs < fs:\n",
    "            time_rs = time[:: int(fs / goal_fs)]\n",
    "        else:\n",
    "            time_rs = np.arange(time[0], time[-1], 1 / goal_fs)\n",
    "    else:\n",
    "        goal_fs = 1 / np.mean(np.diff(time_rs[:5000]))\n",
    "        # prevent t_rs from extrapolating\n",
    "        time_rs = time_rs[time_rs <= time[-1]]\n",
    "\n",
    "    # AA filter, if necessary\n",
    "    if (fs / goal_fs) >= 1.0:\n",
    "        sos = cheby1(8, 0.05, 0.8 / (fs / goal_fs), output=\"sos\")\n",
    "    else:\n",
    "        aa_filter = False\n",
    "\n",
    "    # resample data\n",
    "    data_rs = ()\n",
    "\n",
    "    for dat in data:\n",
    "        if dat is None:\n",
    "            data_rs += (None,)\n",
    "        elif dat.ndim in [1, 2]:\n",
    "            data_to_rs = sosfiltfilt(sos, dat, axis=0) if aa_filter else dat\n",
    "            data_rs += resample(data_to_rs, fs / goal_fs, time, time_rs)\n",
    "        else:\n",
    "            raise ValueError(\"Data dimension exceeds 2, or data not understood.\")\n",
    "\n",
    "    # resampling indices\n",
    "    indices_rs = ()\n",
    "    for idx in indices:\n",
    "        if idx is None:\n",
    "            indices_rs += (None,)\n",
    "        elif idx.ndim == 1:\n",
    "            indices_rs += (\n",
    "                np.around(np.interp(time[idx], time_rs, np.arange(time_rs.size))).astype(np.int_),\n",
    "            )\n",
    "        elif idx.ndim == 2:\n",
    "            indices_rs += (np.zeros(idx.shape, dtype=np.int_),)\n",
    "            for i in range(idx.shape[1]):\n",
    "                indices_rs[-1][:, i] = np.around(\n",
    "                    np.interp(\n",
    "                        time[idx[:, i]], time_rs, np.arange(time_rs.size)\n",
    "                    )  # cast to in on insert\n",
    "                )\n",
    "\n",
    "    ret = (time_rs,)\n",
    "    if data_rs != ():\n",
    "        ret += (data_rs,)\n",
    "    if indices_rs != ():\n",
    "        ret += (indices_rs,)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import warn\n",
    "\n",
    "from avro.datafile import DataFileReader\n",
    "from avro.io import DatumReader\n",
    "from numpy import (\n",
    "    round,\n",
    "    arange,\n",
    "    vstack,\n",
    "    ascontiguousarray,\n",
    "    isclose,\n",
    "    full,\n",
    "    argmin,\n",
    "    abs,\n",
    "    nan,\n",
    "    float64,\n",
    ")\n",
    "\n",
    "class ReadEmpaticaAvro():\n",
    "    \"\"\"\n",
    "    Read Empatica data from an avro file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trim_keys : {None, tuple}, optional\n",
    "        Trim keys provided in the `predict` method. Default (None) will not do any trimming.\n",
    "        Trimming of either start or end can be accomplished by providing None in the place\n",
    "        of the key you do not want to trim. If provided, the tuple should be of the form\n",
    "        (start_key, end_key). When provided, trim datetimes will be assumed to be in the\n",
    "        same timezone as the data (ie naive if naive, or in the timezone provided).\n",
    "    resample_to_accel : bool, optional\n",
    "        Resample any additional data streams to match the accelerometer data stream.\n",
    "        Default is True.\n",
    "    \"\"\"\n",
    "\n",
    "    _file = \"file\"\n",
    "    _time = \"time\"\n",
    "    _acc = \"acc\"\n",
    "    _gyro = \"gyro\"\n",
    "    _mag = \"magnet\"\n",
    "    _temp = \"temperature\"\n",
    "    _days = \"day_ends\"\n",
    "\n",
    "    def __init__(self, trim_keys=None, resample_to_bvp=True):\n",
    "        \n",
    "        self.trim_keys = trim_keys\n",
    "        self.resample_to_bvp = resample_to_bvp\n",
    "\n",
    "    def get_accel(self, raw_accel_dict, results_dict, key):\n",
    "        \"\"\"\n",
    "        Get the raw acceleration data from the avro file record.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_accel_dict : dict\n",
    "            The record from the avro file for a raw data stream.\n",
    "        results_dict : dict\n",
    "            Dictionary where the results will go.\n",
    "        key : str\n",
    "            Name for the results in `results_dict`.\n",
    "        \"\"\"\n",
    "        # sampling frequency\n",
    "        fs = raw_accel_dict[\"samplingFrequency\"]\n",
    "\n",
    "        # timestamp start\n",
    "        ts_start = raw_accel_dict[\"timestampStart\"] / 1e6  # convert to seconds\n",
    "\n",
    "        # imu parameters for scaling to actual values\n",
    "        phys_min = raw_accel_dict[\"imuParams\"][\"physicalMin\"]\n",
    "        phys_max = raw_accel_dict[\"imuParams\"][\"physicalMax\"]\n",
    "        dig_min = raw_accel_dict[\"imuParams\"][\"digitalMin\"]\n",
    "        dig_max = raw_accel_dict[\"imuParams\"][\"digitalMax\"]\n",
    "\n",
    "        # raw acceleration data\n",
    "        accel = ascontiguousarray(\n",
    "            vstack((raw_accel_dict[\"x\"], raw_accel_dict[\"y\"], raw_accel_dict[\"z\"])).T\n",
    "        )\n",
    "\n",
    "        # scale the raw acceleration data to actual values\n",
    "        accel = (accel - dig_min) / (dig_max - dig_min) * (\n",
    "            phys_max - phys_min\n",
    "        ) + phys_min\n",
    "\n",
    "        # create the timestamp array using ts_start, fs, and the number of samples\n",
    "        time = arange(ts_start, ts_start + accel.shape[0] / fs, 1 / fs)[\n",
    "            : accel.shape[0]\n",
    "        ]\n",
    "\n",
    "        if time.size != accel.shape[0]:\n",
    "            raise ValueError(\"Time does not have enough samples for accel array\")\n",
    "\n",
    "        # use special names here so we can just update dictionary later for returning\n",
    "        results_dict[key] = {self._time: time, \"fs\": fs, \"values\": accel}\n",
    "    \n",
    "    def get_bvp(self, raw_bvp_dict, results_dict, key):\n",
    "        \"\"\"\n",
    "        Get the raw blood volume pulse data from the avro file record.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_bvp_dict : dict\n",
    "            The record from the avro file for a raw data stream.\n",
    "        results_dict : dict\n",
    "            Dictionary where the results will go.\n",
    "        key : str\n",
    "            Name for the results in `results_dict`.\n",
    "        \"\"\"\n",
    "        # sampling frequency\n",
    "        fs = round(raw_bvp_dict[\"samplingFrequency\"], decimals=3)\n",
    "        # timestamp start\n",
    "        ts_start = raw_bvp_dict[\"timestampStart\"] / 1e6 # convert to seconds\n",
    "\n",
    "        # raw bvp data\n",
    "        bvp = ascontiguousarray(raw_bvp_dict[\"values\"])\n",
    "\n",
    "        # timestamp array\n",
    "        time = arange(ts_start, ts_start + bvp.size / fs, 1 / fs)[: bvp.shape[0]]\n",
    "\n",
    "        if time.size != bvp.shape[0]:\n",
    "            raise ValueError(\"Time does not have enough samples for bvp array\")\n",
    "        \n",
    "        results_dict[key] = {self._time: time, \"fs\": fs, \"bvp\": bvp}\n",
    "\n",
    "    def get_eda(self, raw_dict, results_dict, key):\n",
    "        \"\"\"\n",
    "        Get the raw electrodermal activity data from the avro file record.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_dict : dict\n",
    "            The record from the avro file for a raw data stream.\n",
    "        results_dict : dict\n",
    "            Dictionary where the results will go.\n",
    "        key : str\n",
    "            Name for the results in `results_dict`.\n",
    "        \"\"\"\n",
    "        if not raw_dict[\"values\"]:\n",
    "            return\n",
    "\n",
    "        # sampling frequency\n",
    "        fs = round(raw_dict[\"samplingFrequency\"], decimals=3)\n",
    "        # timestamp start\n",
    "        ts_start = raw_dict[\"timestampStart\"] / 1e6\n",
    "\n",
    "        # raw eda data\n",
    "        eda = ascontiguousarray(raw_dict[\"values\"])\n",
    "\n",
    "        # timestamp array\n",
    "        time = arange(ts_start, ts_start + eda.size / fs, 1 / fs)[: eda.shape[0]]\n",
    "\n",
    "        if time.size != eda.shape[0]:\n",
    "            raise ValueError(\"Time does not have enough samples for eda array\")\n",
    "        \n",
    "        results_dict[key] = {\"time_eda\": time, \"fs_eda\": fs, \"eda\": eda}\n",
    "\n",
    "    def get_temp(self, raw_dict, results_dict, key):\n",
    "        \"\"\"\n",
    "        Get the raw temperature data from the avro file record.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_dict : dict\n",
    "            The record from the avro file for a raw data stream.\n",
    "        results_dict : dict\n",
    "            Dictionary where the results will go.\n",
    "        key : str\n",
    "            Name for the results in `results_dict`.\n",
    "        \"\"\"\n",
    "        if not raw_dict[\"values\"]:\n",
    "            return\n",
    "\n",
    "        # sampling frequency\n",
    "        fs = round(raw_dict[\"samplingFrequency\"], decimals=3)\n",
    "        # timestamp start\n",
    "        ts_start = raw_dict[\"timestampStart\"] / 1e6\n",
    "\n",
    "        # raw temperature data\n",
    "        temp = ascontiguousarray(raw_dict[\"values\"])\n",
    "\n",
    "        # timestamp array\n",
    "        time = arange(ts_start, ts_start + temp.size / fs, 1 / fs)[: temp.shape[0]]\n",
    "\n",
    "        if time.size != temp.shape[0]:\n",
    "            raise ValueError(\"Time does not have enough samples for temp array\")\n",
    "        \n",
    "        results_dict[key] = {\"time_temp\": time, \"fs_temp\": fs, \"temp\": temp}\n",
    "\n",
    "    def get_values_1d(self, raw_dict, results_dict, key):\n",
    "        \"\"\"\n",
    "        Get the raw 1-dimensional values data from the avro file record.\n",
    "        i.e, PPG, EDA, and temperature\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_dict : dict\n",
    "            The record from the avro file for a raw 1-dimensional values data stream.\n",
    "        results_dict : dict\n",
    "            Dictionary where the results will go.\n",
    "        key : str\n",
    "            Name for the results in `results_dict`.\n",
    "        \"\"\"\n",
    "        if not raw_dict[\"values\"]:\n",
    "            return\n",
    "\n",
    "        # sampling frequency\n",
    "        fs = round(raw_dict[\"samplingFrequency\"], decimals=3)\n",
    "        # timestamp start\n",
    "        ts_start = raw_dict[\"timestampStart\"] / 1e6  # convert to seconds\n",
    "\n",
    "        # raw values data\n",
    "        values = ascontiguousarray(raw_dict[\"values\"])\n",
    "\n",
    "        # timestamp array\n",
    "        time = arange(ts_start, ts_start + values.size / fs, 1 / fs)[: values.shape[0]]\n",
    "\n",
    "        if time.size != values.shape[0]:\n",
    "            raise ValueError(f\"Time does not have enough samples for {key} array\")\n",
    "\n",
    "        results_dict[key] = {self._time: time, \"fs\": fs, \"values\": values}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_systolic_peaks(raw_dict, results_dict, key):\n",
    "        \"\"\"\n",
    "        Get the systolic peaks data from the avro file record.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_dict : dict\n",
    "            The record from the avro file for systolic peaks data.\n",
    "        results_dict : dict\n",
    "            Dictionary where the results will go.\n",
    "        key : str\n",
    "            Name for the results in `results_dict`.\n",
    "        \"\"\"\n",
    "        if not raw_dict[\"peaksTimeNanos\"]:\n",
    "            return\n",
    "\n",
    "        peaks = (\n",
    "            ascontiguousarray(raw_dict[\"peaksTimeNanos\"]) / 1e9\n",
    "        )  # convert to seconds\n",
    "\n",
    "        results_dict[key] = {\"values\": peaks}\n",
    "\n",
    "    def get_steps(self, raw_dict, results_dict, key):\n",
    "        \"\"\"\n",
    "        Get the raw steps data from the avro file record.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_dict : dict\n",
    "            The record from the avro file for raw steps data.\n",
    "        results_dict : dict\n",
    "            Dictionary where the results will go.\n",
    "        key : str\n",
    "            Name for the results in `results_dict`.\n",
    "        \"\"\"\n",
    "        if not raw_dict[\"values\"]:\n",
    "            return\n",
    "\n",
    "        # sampling frequency\n",
    "        fs = round(raw_dict[\"samplingFrequency\"], decimals=3)\n",
    "\n",
    "        # timestamp start\n",
    "        ts_start = raw_dict[\"timestampStart\"] / 1e6  # convert to seconds\n",
    "\n",
    "        # raw steps data\n",
    "        steps = ascontiguousarray(raw_dict[\"values\"])\n",
    "\n",
    "        # timestamp array\n",
    "        time = arange(ts_start, ts_start + steps.size / fs, 1 / fs)[: steps.size]\n",
    "\n",
    "        if time.size != steps.size:\n",
    "            raise ValueError(\"Time does not have enough samples for steps array\")\n",
    "\n",
    "        results_dict[key] = {self._time: time, \"fs\": fs, \"values\": steps}\n",
    "\n",
    "    def handle_resampling(self, streams):\n",
    "        \"\"\"\n",
    "        Handle resampling of data streams. Data will be resampled to match the\n",
    "        BVP (Blood Volume Pulse) data stream.\n",
    "        \"\"\"\n",
    "        if \"bvp\" not in streams:\n",
    "            raise ValueError(\"BVP data stream is missing, cannot resample.\")\n",
    "        \n",
    "        # Remove BVP data stream\n",
    "        bvp_dict = streams.pop(\"bvp\")\n",
    "        # Remove Temp data stream\n",
    "        temp_dict = streams.pop(\"temperature\")\n",
    "        # Remove EDA data stream\n",
    "        eda_dict = streams.pop(\"eda\")\n",
    "\n",
    "        # Remove keys that cannot be resampled\n",
    "        rs_streams = {d: streams.pop(d) for d in [\"systolic_peaks\", \"steps\"] if d in streams}\n",
    "        \n",
    "        for name, stream in streams.items():\n",
    "            if stream[\"values\"] is None:\n",
    "                continue\n",
    "            \n",
    "            # Check that the stream doesn't start significantly later than BVP\n",
    "            # if (dt := (stream[\"time\"][0] - bvp_dict[\"time\"][0])) > 1:\n",
    "            #     warn(\n",
    "            #         f\"Data stream {name} starts more than 1 second ({dt}s) after \"\n",
    "            #         f\"the BVP stream. Data will be filled with the first (and \"\n",
    "            #         f\"last) value as needed.\"\n",
    "            #     )\n",
    "            \n",
    "            # Check if resampling is needed\n",
    "            if isclose(stream[\"fs\"], bvp_dict[\"fs\"], atol=1e-3):\n",
    "                new_shape = list(stream[\"values\"].shape)\n",
    "                new_shape[0] = bvp_dict[\"bvp\"].shape[0]\n",
    "                rs_streams[name] = full(new_shape, nan, dtype=float64)\n",
    "                i1 = argmin(abs(bvp_dict[\"time\"] - stream[\"time\"][0]))\n",
    "                i2 = i1 + stream[\"time\"].size\n",
    "                rs_streams[name][i1:i2] = stream[\"values\"][: stream[\"values\"].shape[0] - (i2 - bvp_dict[\"time\"].size)]\n",
    "                rs_streams[name][:i1] = stream[\"values\"][0]\n",
    "                rs_streams[name][i2:] = stream[\"values\"][-1]\n",
    "                continue\n",
    "            \n",
    "            # Resample the stream to match BVP\n",
    "            _, (stream_rs,) = apply_resample(\n",
    "                time=stream[\"time\"],\n",
    "                time_rs=bvp_dict[\"time\"],\n",
    "                data=(stream[\"values\"],),\n",
    "                aa_filter=True,\n",
    "                fs=stream[\"fs\"],\n",
    "            )\n",
    "            rs_streams[name] = stream_rs\n",
    "        \n",
    "        rs_streams.update(bvp_dict)\n",
    "        rs_streams.update(temp_dict)\n",
    "        rs_streams.update(eda_dict)\n",
    "        return rs_streams\n",
    "\n",
    "    def get_datastreams(self, raw_record):\n",
    "        \"\"\"\n",
    "        Extract the various data streams from the raw avro file record.\n",
    "        \"\"\"\n",
    "        fn_map = {\n",
    "            \"accelerometer\": (\"acc\", self.get_accel),\n",
    "            \"eda\": (\"eda\", self.get_eda),\n",
    "            \"temperature\": (\"temperature\", self.get_temp),\n",
    "            \"bvp\": (\"bvp\", self.get_bvp),\n",
    "            \"systolicPeaks\": (\"systolic_peaks\", self.get_systolic_peaks),\n",
    "            \"steps\": (\"steps\", self.get_steps),\n",
    "        }\n",
    "\n",
    "        raw_data_streams = {}\n",
    "        for full_name, (stream_name, fn) in fn_map.items():\n",
    "            fn(raw_record[full_name], raw_data_streams, stream_name)\n",
    "        \n",
    "        if self.resample_to_bvp:\n",
    "            data_streams = self.handle_resampling(raw_data_streams)\n",
    "        else:\n",
    "            data_streams = raw_data_streams.pop(\"bvp\")\n",
    "            data_streams.update(raw_data_streams)\n",
    "        \n",
    "        return data_streams\n",
    "\n",
    "    def read(self, *, file, tz_name=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Read the input .avro file.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file : {path-like, str}\n",
    "            The path to the input file.\n",
    "        tz_name : {None, optional}\n",
    "            IANA time-zone name for the recording location. If not provided, timestamps\n",
    "            will represent local time naively. This means they will not account for\n",
    "            any time changes due to Daylight Saving Time.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        results : dict\n",
    "            Dictionary containing the data streams from the file. See Notes\n",
    "            for different output options.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        There are two output formats, based on if `resample_to_accel` is True or False.\n",
    "        If True, all available data streams except for `systolic_peaks` and `steps`\n",
    "        are resampled to match the accelerometer data stream, which results in their\n",
    "        values being present in the top level of the `results` dictionary, ie\n",
    "        `results['gyro']`, etc.\n",
    "\n",
    "        If False, everything except accelerometer will be present in dictionaries\n",
    "        containing the keys `time`, `fs`, and `values`, and the top level will be these\n",
    "        dictionaries plus the accelerometer data (keys `time`, `fs`, and `accel`).\n",
    "\n",
    "        `systolic_peaks` will always be a dictionary of the form `{'systolic_peaks': array}`.\n",
    "        \"\"\"\n",
    "\n",
    "        reader = DataFileReader(open(file, \"rb\"), DatumReader())\n",
    "        records = []\n",
    "        for record in reader:\n",
    "            records.append(record)\n",
    "        reader.close()\n",
    "\n",
    "        # get the timezone offset\n",
    "        tz_offset = records[0][\"timezone\"]  # in seconds\n",
    "\n",
    "        # as needed, deviceSn, deviceModel\n",
    "\n",
    "        # get the data streams\n",
    "        results = self.get_datastreams(records[0][\"rawData\"])\n",
    "\n",
    "        # update the timestamps to be local. Do this as we don't have an actual\n",
    "        # timezone from the data.\n",
    "        if tz_name is None:\n",
    "            results[\"time\"] += tz_offset\n",
    "            results[\"time_temp\"] += tz_offset\n",
    "            results[\"time_eda\"] += tz_offset\n",
    "\n",
    "            for k in results:\n",
    "                if k == \"time\":\n",
    "                    continue\n",
    "                if (\n",
    "                    isinstance(results[k], dict)\n",
    "                    and \"time\" in results[k]\n",
    "                    and results[k][\"time\"] is not None\n",
    "                ):\n",
    "                    results[k][\"time\"] += tz_offset\n",
    "        # do nothing if we have the time-zone name, the timestamps are already\n",
    "        # UTC\n",
    "\n",
    "        # adjust systolic_peaks\n",
    "        if \"systolic_peaks\" in results:\n",
    "            results[\"systolic_peaks\"][\"values\"] += tz_offset\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "empatica_reader = ReadEmpaticaAvro()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Acc for GGIR\n",
    "~ 2 min for reading and concatenating into a list\n",
    "\n",
    "**With Pandas**\n",
    "\n",
    "- ~ 2.30 min for transforming it into a pd.DataFrame\n",
    "- ~ 1.30 min for saving to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing day 1/8\n",
      "Processing day 2/8\n",
      "Processing day 3/8\n",
      "Processing day 4/8\n",
      "Processing day 5/8\n",
      "Processing day 6/8\n",
      "Processing day 7/8\n",
      "Processing day 8/8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "#### Change the paths below to the location of the data on your machine ####\n",
    "data_path = \"/Users/augenpro/Documents/Empatica/data_sara/data/participant_data/\"\n",
    "save_data_path = \"/Users/augenpro/Documents/Empatica/data_sara/data/GGIR_input/\"\n",
    "#### Change the subject ID and device ID below to the subject and device you want to process ####\n",
    "sub_ID = \"00007\"\n",
    "device_ID = \"3YK3J151VJ\"\n",
    "\n",
    "days = sorted(os.listdir(data_path))\n",
    "days = [day for day in days if day[0] != \".\"] # remove hidden files (needed for MacOS users)\n",
    "\n",
    "acc = []\n",
    "ppg = []\n",
    "temp = []\n",
    "time = []\n",
    "time_temp = []\n",
    "\n",
    "for i, day in enumerate(days):\n",
    "    \n",
    "    print(f\"Processing day {i+1}/{len(days)}\")\n",
    "\n",
    "    folder_day = data_path + day + \"/\" + sub_ID + \"-\" + device_ID + \"/raw_data/v6\"\n",
    "\n",
    "    avro_files = sorted(glob.glob(folder_day + \"/*.avro\"))\n",
    "\n",
    "    for avro_file in avro_files:\n",
    "        \n",
    "        data = empatica_reader.read(file=avro_file)\n",
    "\n",
    "        acc.extend(data[\"acc\"])\n",
    "\n",
    "        ppg.extend(data[\"bvp\"])\n",
    "\n",
    "        temp.extend(data[\"temp\"])\n",
    "        time_temp.extend(data[\"time_temp\"])\n",
    "\n",
    "        time.extend(data[\"time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect non-wear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df = pd.DataFrame(acc, columns=[\"x\", \"y\", \"z\"], index = pd.to_datetime(time, unit=\"s\")).sort_index()\n",
    "ppg_df = pd.DataFrame(ppg, columns=[\"ppg\"], index = pd.to_datetime(time, unit=\"s\")).sort_index()\n",
    "temp_df = pd.DataFrame(temp, columns=[\"temp\"], index = pd.to_datetime(time_temp, unit=\"s\")).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to divide it into portions when the device was in charge\n",
    "\n",
    "t_charge_end = acc_df.index[acc_df.index.to_series().diff().dt.total_seconds() > 60*10]\n",
    "t_charge_start = acc_df.index[np.where(acc_df.index.to_series().diff().dt.total_seconds() > 60*10)[0]-1]\n",
    "t_charge = pd.DataFrame({\"start\": t_charge_start, \"end\": t_charge_end})\n",
    "\n",
    "good_portions = pd.DataFrame(columns=[\"start\", \"end\"])\n",
    "good_portions[\"start\"] = t_charge[\"end\"].iloc[:-1].reset_index(drop=True)\n",
    "good_portions[\"end\"] = t_charge[\"start\"].iloc[1:].reset_index(drop=True)\n",
    "start_first_charge = t_charge[\"start\"].iloc[0]\n",
    "end_last_charge = t_charge[\"end\"].iloc[-1]\n",
    "\n",
    "# Segment the data into portions when the device was not in charge and perform nonwear detection\n",
    "acc_df_portions = [acc_df[:start_first_charge]]\n",
    "ppg_df_portions = [ppg_df[:start_first_charge]]\n",
    "temp_df_portions = [temp_df[:start_first_charge]]\n",
    "\n",
    "for i, row in good_portions.iterrows():\n",
    "\n",
    "    if row[\"end\"] - row[\"start\"] < pd.Timedelta(\"10 min\"): # if the portion is less than 10 minutes, skip it\n",
    "        continue\n",
    "\n",
    "    acc_df_portions.append(acc_df[row[\"start\"]:row[\"end\"]])\n",
    "    ppg_df_portions.append(ppg_df[row[\"start\"]:row[\"end\"]])\n",
    "    temp_df_portions.append(temp_df[row[\"start\"]:row[\"end\"]])\n",
    "\n",
    "acc_df_portions.append(acc_df[end_last_charge:])\n",
    "ppg_df_portions.append(ppg_df[end_last_charge:])\n",
    "temp_df_portions.append(temp_df[end_last_charge:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nonwear.DETACH import nimbaldetach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each of them, perform NW detection\n",
    "acc_df_cleaned = []\n",
    "temp_df_cleaned = []\n",
    "ppg_df_cleaned = []\n",
    "for i, (acc, temp, ppg) in enumerate(zip(acc_df_portions, temp_df_portions, ppg_df_portions)):\n",
    "\n",
    "    start_stop_nw, _ = nimbaldetach(acc['x'].values, acc['y'].values, acc['z'].values, temp[\"temp\"].values, accel_freq=64, temperature_freq=1, quiet=True)\n",
    "\n",
    "    # Remove non-wear periods\n",
    "    for i, row in start_stop_nw.iterrows():\n",
    "        datetime_start_nw = acc.index[row[\"Start Datapoint\"]]\n",
    "        datetime_end_nw = acc.index[row[\"End Datapoint\"]]\n",
    "        acc.loc[datetime_start_nw:datetime_end_nw] = np.nan\n",
    "        temp.loc[datetime_start_nw:datetime_end_nw] = np.nan\n",
    "        ppg.loc[datetime_start_nw:datetime_end_nw] = np.nan\n",
    "\n",
    "    acc_portion = acc.dropna()\n",
    "    temp_portion = temp.dropna()\n",
    "    ppg_portion = ppg.dropna()\n",
    "\n",
    "    plt.figure(figsize=(19, 11))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(acc_portion.resample(\"20s\").mean(), label=\"acc\")\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 1, 2, sharex = plt.subplot(2, 1, 1))\n",
    "    plt.plot(temp_portion.resample(\"30 s\").mean(), label=\"temp\")\n",
    "    plt.legend()\n",
    "\n",
    "    acc_df_cleaned.append(acc_portion)\n",
    "    temp_df_cleaned.append(temp_portion)\n",
    "    ppg_df_cleaned.append(ppg_portion)\n",
    "\n",
    "acc_df_cleaned = pd.concat(acc_df_cleaned)\n",
    "temp_df_cleaned = pd.concat(temp_df_cleaned)\n",
    "ppg_df_cleaned = pd.concat(ppg_df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv for GGIR\n",
    "acc_df.to_csv(save_data_path + \"/acc_new.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x31b6153d0>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(19, 11))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(acc_df.resample(\"30s\").mean())\n",
    "plt.subplot(3, 1, 2, sharex = plt.subplot(3, 1, 1))\n",
    "plt.plot(temp_df.resample(\"30s\").mean())\n",
    "plt.subplot(3, 1, 3, sharex = plt.subplot(3, 1, 1))\n",
    "plt.plot(ppg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x43ae282f0>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(19, 11))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc_df.values)\n",
    "plt.subplot(2, 1, 2, sharex = plt.subplot(2, 1, 1))\n",
    "plt.plot(ppg_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to parquet\n",
    "acc_df.to_parquet(save_data_path + \"/acc.parquet\")\n",
    "temp_df.to_parquet(save_data_path + \"/temp.parquet\")\n",
    "ppg_df.to_parquet(save_data_path + \"/ppg.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GGIR output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_GGIR_path = \"/Users/augenpro/Documents/Empatica/data_sara/data/GGIR_output/output_GGIR_input/results/QC/\"\n",
    "\n",
    "output_GGIR = pd.read_csv(output_GGIR_path + \"part4_nightsummary_sleep_full.csv\")\n",
    "\n",
    "SPT = []\n",
    "\n",
    "for i, day_row in output_GGIR.iterrows():\n",
    "    # Stupid thing to get the correct datetime for segmenting signals into day and night (but no alternatives I guess)\n",
    "    if output_GGIR[\"sleeponset_ts\"].iloc[0][0] == '0':\n",
    "        sleep_onset = pd.to_datetime(str(pd.to_datetime(day_row[\"calendar_date\"]).date() + pd.Timedelta(\"1d\")) + \" \" + day_row[\"sleeponset_ts\"])\n",
    "    else:\n",
    "        sleep_onset = pd.to_datetime(pd.to_datetime(day_row[\"calendar_date\"]).date() + \" \" + day_row[\"sleeponset_ts\"])\n",
    "\n",
    "    wake_onset = pd.to_datetime(str(pd.to_datetime(day_row[\"calendar_date\"]).date() + pd.Timedelta(\"1d\")) + \" \" + day_row[\"wakeup_ts\"])\n",
    "\n",
    "    SPT.append((sleep_onset, wake_onset))\n",
    "\n",
    "start_end_sleep = np.array(SPT).reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[Timestamp('2024-05-21 01:02:45'),\n",
       "        Timestamp('2024-05-21 08:07:30')],\n",
       "       [Timestamp('2024-05-22 00:15:30'),\n",
       "        Timestamp('2024-05-22 05:59:55')],\n",
       "       [Timestamp('2024-05-23 00:15:30'),\n",
       "        Timestamp('2024-05-23 06:10:10')],\n",
       "       [Timestamp('2024-05-25 01:03:10'),\n",
       "        Timestamp('2024-05-25 06:07:55')],\n",
       "       [Timestamp('2024-05-26 23:48:50'),\n",
       "        Timestamp('2024-05-26 09:49:05')],\n",
       "       [Timestamp('2024-05-27 23:39:40'),\n",
       "        Timestamp('2024-05-27 06:14:20')]], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_end_sleep[1,0] = pd.Timestamp('2024-05-22 00:15:30')\n",
    "start_end_sleep[2,0] = pd.Timestamp('2024-05-23 00:15:30')\n",
    "# remove the fourth night\n",
    "start_end_sleep = np.delete(start_end_sleep, 3, axis=0)\n",
    "start_end_sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[Timestamp('2024-05-21 01:02:45'),\n",
       "        Timestamp('2024-05-21 08:07:30')],\n",
       "       [Timestamp('2024-05-22 00:15:30'),\n",
       "        Timestamp('2024-05-22 05:59:55')],\n",
       "       [Timestamp('2024-05-23 00:15:30'),\n",
       "        Timestamp('2024-05-23 06:10:10')],\n",
       "       [Timestamp('2024-05-24 00:30:30'),\n",
       "        Timestamp('2024-05-24 07:40:30')],\n",
       "       [Timestamp('2024-05-26 00:00:30'),\n",
       "        Timestamp('2024-05-26 09:30:30')],\n",
       "       [Timestamp('2024-05-26 23:45:30'),\n",
       "        Timestamp('2024-05-27 06:05:30')]], dtype=object)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_end_sleep[3,0] = pd.Timestamp('2024-05-24 00:30:30')\n",
    "start_end_sleep[3,1] = pd.Timestamp('2024-05-24 07:40:30')\n",
    "start_end_sleep[4,0] = pd.Timestamp('2024-05-26 00:00:30')\n",
    "start_end_sleep[4,1] = pd.Timestamp('2024-05-26 09:30:30')\n",
    "start_end_sleep[5,0] = pd.Timestamp('2024-05-26 23:45:30')\n",
    "start_end_sleep[5,1] = pd.Timestamp('2024-05-27 06:05:30')\n",
    "\n",
    "start_end_sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract night HRV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment night data\n",
    "plt.figure(figsize=(19, 11))\n",
    "plt.plot(acc_df.resample(\"1s\").mean())\n",
    "for start_sleep, end_sleep in start_end_sleep:\n",
    "    plt.axvspan(start_sleep, end_sleep, color=\"red\", alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc_SMV(acc_df):\n",
    "    return np.sqrt(acc_df[\"x\"]**2 + acc_df[\"y\"]**2 + acc_df[\"z\"]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sleep.acc_bursts.detect_acc_bursts import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "def ppg_findpeaks_charlton(\n",
    "    signal,\n",
    "    sampling_rate=1000,\n",
    "    show=False,\n",
    "):\n",
    "    \"\"\"Implementation of Charlton et al (2024) MSPTDfast: An Efficient Photoplethysmography\n",
    "    Beat Detection Algorithm. 2024 Computing in Cardiology (CinC), Karlsruhe, Germany,\n",
    "    doi:10.1101/2024.07.18.24310627.\n",
    "    \"\"\"\n",
    "\n",
    "    # Inner functions\n",
    "\n",
    "    def find_m_max(x, N, max_scale, m_max):\n",
    "        \"\"\"Find local maxima scalogram for peaks\n",
    "        \"\"\"\n",
    "\n",
    "        for k in range(1, max_scale + 1):  # scalogram scales\n",
    "            for i in range(k + 2, N - k + 2):\n",
    "                if x[i - 2] > x[i - k - 2] and x[i - 2] > x[i + k - 2]:\n",
    "                    m_max[k - 1, i - 2] = True\n",
    "\n",
    "        return m_max\n",
    "\n",
    "    def find_m_min(x, N, max_scale, m_min):\n",
    "        \"\"\"Find local minima scalogram for onsets\n",
    "        \"\"\"\n",
    "\n",
    "        for k in range(1, max_scale + 1):  # scalogram scales\n",
    "            for i in range(k + 2, N - k + 2):\n",
    "                if x[i - 2] < x[i - k - 2] and x[i - 2] < x[i + k - 2]:\n",
    "                    m_min[k - 1, i - 2] = True\n",
    "\n",
    "        return m_min\n",
    "\n",
    "    def find_lms_using_msptd_approach(max_scale, x, options):\n",
    "        \"\"\"Find local maxima (or minima) scalogram(s) using the\n",
    "        MSPTD approach\n",
    "        \"\"\"\n",
    "\n",
    "        # Setup\n",
    "        N = len(x)\n",
    "\n",
    "        # Find local maxima scalogram (if required)\n",
    "        if options[\"find_pks\"]:\n",
    "            m_max = np.full((max_scale, N), False)  # matrix for maxima\n",
    "            m_max = find_m_max(x, N, max_scale, m_max)\n",
    "        else:\n",
    "            m_max = None\n",
    "\n",
    "        # Find local minima scalogram (if required)\n",
    "        if options[\"find_trs\"]:\n",
    "            m_min = np.full((max_scale, N), False)  # matrix for minima\n",
    "            m_min = find_m_min(x, N, max_scale, m_min)\n",
    "        else:\n",
    "            m_min = None\n",
    "\n",
    "        return m_max, m_min\n",
    "\n",
    "    def downsample(win_sig, ds_factor):\n",
    "        \"\"\"Downsamples signal by picking out every nth sample, where n is\n",
    "        specified by ds_factor\n",
    "        \"\"\"\n",
    "\n",
    "        return win_sig[::ds_factor]\n",
    "\n",
    "    def detect_peaks_and_onsets_using_msptd(signal, fs, options):\n",
    "        \"\"\"Detect peaks and onsets in a PPG signal using a modified MSPTD approach\n",
    "        (where the modifications are those specified in Charlton et al. 2024)\n",
    "        \"\"\"\n",
    "\n",
    "        # Setup\n",
    "        N = len(signal)\n",
    "        L = int(np.ceil(N / 2) - 1)\n",
    "\n",
    "        # Step 0: Don't calculate scales outside the range of plausible HRs\n",
    "\n",
    "        plaus_hr_hz = np.array(options['plaus_hr_bpm']) / 60  # in Hz\n",
    "        init_scales = np.arange(1, L + 1)\n",
    "        durn_signal = len(signal) / fs\n",
    "        init_scales_fs = (L / init_scales) / durn_signal\n",
    "        if options['use_reduced_lms_scales']:\n",
    "            init_scales_inc_log = init_scales_fs >= plaus_hr_hz[0]\n",
    "        else:\n",
    "            init_scales_inc_log = np.ones_like(init_scales_fs, dtype=bool)  # DIDN\"T FULLY UNDERSTAND\n",
    "\n",
    "        max_scale_index = np.where(init_scales_inc_log)[0]  # DIDN\"T FULLY UNDERSTAND THIS AND NEXT FEW LINES\n",
    "        if max_scale_index.size > 0:\n",
    "            max_scale = max_scale_index[-1] + 1  # Add 1 to convert from 0-based to 1-based index\n",
    "        else:\n",
    "            max_scale = None  # Or handle the case where no scales are valid\n",
    "\n",
    "        # Step 1: calculate local maxima and local minima scalograms\n",
    "\n",
    "        # - detrend\n",
    "        x = scipy.signal.detrend(signal, type=\"linear\")\n",
    "\n",
    "        # - populate LMS matrices\n",
    "        [m_max, m_min] = find_lms_using_msptd_approach(max_scale, x, options)\n",
    "\n",
    "        # Step 2: find the scale with the most local maxima (or local minima)\n",
    "\n",
    "        # - row-wise summation (i.e. sum each row)\n",
    "        if options[\"find_pks\"]:\n",
    "            gamma_max = np.sum(m_max, axis=1)  # the \"axis=1\" option makes it row-wise\n",
    "        if options[\"find_trs\"]:\n",
    "            gamma_min = np.sum(m_min, axis=1)\n",
    "        # - find scale with the most local maxima (or local minima)\n",
    "        if options[\"find_pks\"]:\n",
    "            lambda_max = np.argmax(gamma_max)\n",
    "        if options[\"find_trs\"]:\n",
    "            lambda_min = np.argmax(gamma_min)\n",
    "\n",
    "        # Step 3: Use lambda to remove all elements of m for which k>lambda\n",
    "        first_scale_to_include = np.argmax(init_scales_inc_log)\n",
    "        if options[\"find_pks\"]:\n",
    "            m_max = m_max[first_scale_to_include:lambda_max + 1, :]\n",
    "        if options[\"find_trs\"]:\n",
    "            m_min = m_min[first_scale_to_include:lambda_min + 1, :]\n",
    "\n",
    "        # Step 4: Find peaks (and onsets)\n",
    "        # - column-wise summation\n",
    "        if options[\"find_pks\"]:\n",
    "            m_max_sum = np.sum(m_max == False, axis=0)\n",
    "            peaks = np.where(m_max_sum == 0)[0].astype(int)\n",
    "        else:\n",
    "            peaks = []\n",
    "\n",
    "        if options[\"find_trs\"]:\n",
    "            m_min_sum = np.sum(m_min == False, axis=0)\n",
    "            onsets = np.where(m_min_sum == 0)[0].astype(int)\n",
    "        else:\n",
    "            onsets = []\n",
    "\n",
    "        return peaks, onsets\n",
    "\n",
    "    # ~~~ Main function ~~~\n",
    "\n",
    "    # Specify settings\n",
    "    # - version: optimal selection (CinC 2024)\n",
    "    options = {\n",
    "        'find_trs': True,  # whether or not to find onsets\n",
    "        'find_pks': True,  # whether or not to find peaks\n",
    "        'do_ds': True,  # whether or not to do downsampling\n",
    "        'ds_freq': 20,  # the target downsampling frequency\n",
    "        'use_reduced_lms_scales': True,  # whether or not to reduce the number of scales (default 30 bpm)\n",
    "        'win_len': 8,  # duration of individual windows for analysis\n",
    "        'win_overlap': 0.2,  # proportion of window overlap\n",
    "        'plaus_hr_bpm': [30, 200]  # range of plausible HRs (only the lower bound is used)\n",
    "    }\n",
    "\n",
    "    # Split into overlapping windows\n",
    "    no_samps_in_win = options[\"win_len\"] * sampling_rate\n",
    "    if len(signal) <= no_samps_in_win:\n",
    "        win_starts = 0\n",
    "        win_ends = len(signal) - 1\n",
    "    else:\n",
    "        win_offset = int(round(no_samps_in_win * (1 - options[\"win_overlap\"])))\n",
    "        win_starts = list(range(0, len(signal) - no_samps_in_win + 1, win_offset))\n",
    "        win_ends = [start + 1 + no_samps_in_win for start in win_starts]\n",
    "        if win_ends[-1] < len(signal):\n",
    "            win_starts.append(len(signal) - 1 - no_samps_in_win)\n",
    "            win_ends.append(len(signal))\n",
    "        # this ensures that the windows include the entire signal duration\n",
    "\n",
    "    # Set up downsampling if the sampling frequency is particularly high\n",
    "    if options[\"do_ds\"]:\n",
    "        min_fs = options[\"ds_freq\"]\n",
    "        if sampling_rate > min_fs:\n",
    "            ds_factor = int(np.floor(sampling_rate / min_fs))\n",
    "            ds_fs = sampling_rate / np.floor(sampling_rate / min_fs)\n",
    "        else:\n",
    "            options[\"do_ds\"] = False\n",
    "\n",
    "    # detect peaks and onsets in each window\n",
    "    peaks = []\n",
    "    onsets = []\n",
    "\n",
    "    # cycle through each window\n",
    "    for win_no in range(len(win_starts)):\n",
    "        # Extract this window's data\n",
    "        win_sig = signal[win_starts[win_no]:win_ends[win_no]]\n",
    "\n",
    "        # Downsample signal\n",
    "        if options['do_ds']:\n",
    "            rel_sig = downsample(win_sig, ds_factor)\n",
    "            rel_fs = ds_fs\n",
    "        else:\n",
    "            rel_sig = win_sig\n",
    "            rel_fs = sampling_rate\n",
    "\n",
    "        # Detect peaks and onsets\n",
    "        p, t = detect_peaks_and_onsets_using_msptd(rel_sig, rel_fs, options)\n",
    "\n",
    "        # Resample peaks\n",
    "        if options['do_ds']:\n",
    "            p = [peak * ds_factor for peak in p]\n",
    "            t = [onset * ds_factor for onset in t]\n",
    "\n",
    "        # Correct peak indices by finding highest point within tolerance either side of detected peaks\n",
    "        tol_durn = 0.05\n",
    "        if rel_fs < 10:\n",
    "            tol_durn = 0.2\n",
    "        elif rel_fs < 20:\n",
    "            tol_durn = 0.1\n",
    "        tol = int(np.ceil(rel_fs * tol_durn))\n",
    "\n",
    "        for pk_no in range(len(p)):\n",
    "            segment = win_sig[(p[pk_no] - tol):(p[pk_no] + tol + 1)]\n",
    "            temp = np.argmax(segment)\n",
    "            p[pk_no] = p[pk_no] - tol + temp\n",
    "\n",
    "        # Correct onset indices by finding highest point within tolerance either side of detected onsets\n",
    "        for onset_no in range(len(t)):\n",
    "            segment = win_sig[(t[onset_no] - tol):(t[onset_no] + tol + 1)]\n",
    "            temp = np.argmin(segment)\n",
    "            t[onset_no] = t[onset_no] - tol + temp\n",
    "\n",
    "        # Store peaks and onsets\n",
    "        win_peaks = [peak + win_starts[win_no] for peak in p]\n",
    "        peaks.extend(win_peaks)\n",
    "        win_onsets = [onset + win_starts[win_no] for onset in t]\n",
    "        onsets.extend(win_onsets)\n",
    "\n",
    "    # Tidy up detected peaks and onsets (by ordering them and only retaining unique ones)\n",
    "    peaks = sorted(set(peaks))\n",
    "    onsets = sorted(set(onsets))\n",
    "\n",
    "    # Plot results (optional)\n",
    "    if show:\n",
    "        _, ax0 = plt.subplots(nrows=1, ncols=1, sharex=True)\n",
    "        ax0.plot(signal, label=\"signal\")\n",
    "        ax0.scatter(peaks, signal[peaks], c=\"r\")\n",
    "        ax0.scatter(onsets, signal[onsets], c=\"b\")\n",
    "        ax0.set_title(\"PPG Onsets (Method by Charlton et al., 2024)\")\n",
    "\n",
    "    return peaks, onsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ppg_night.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heart_rate.ppg_utils import signal_fixpeaks\n",
    "from heart_rate.hrv.heart_rate_fragmentation import compute_HRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppg_night = []\n",
    "acc_night = []\n",
    "\n",
    "thresold_bursts = 25 # 25 mg\n",
    "td = pd.Timedelta(\"5 s\") # discard 5 sec before and after movement to be sure to have only quiet periods\n",
    "\n",
    "HRV = {}\n",
    "HRV_df = []\n",
    "\n",
    "for i, (start_sleep, end_sleep) in enumerate(start_end_sleep): # for each night\n",
    "\n",
    "    HRV[f\"day {i+1}\"] = []\n",
    "\n",
    "    acc_night = compute_acc_SMV(acc_df.loc[start_sleep:end_sleep])\n",
    "    ppg_night = ppg_df.loc[start_sleep:end_sleep]\n",
    "\n",
    "    # Detect wrist accelerometer bursts\n",
    "    bursts = detect_bursts(acc_night, sampling_rate=64, alfa = thresold_bursts/1000)\n",
    "\n",
    "    # Extract quiet periods (no movement of the wrist)\n",
    "    quiet_periods = pd.DataFrame()\n",
    "    quiet_periods[\"start\"] = bursts[\"end\"].iloc[:-1].reset_index(drop=True)\n",
    "    quiet_periods[\"end\"] = bursts[\"start\"].iloc[1:].reset_index(drop=True)\n",
    "\n",
    "    # plt.figure(figsize=(19, 11))\n",
    "    for j, quiet_period in quiet_periods.iterrows(): # for each quiet period\n",
    "\n",
    "        if quiet_period[\"end\"] - quiet_period[\"start\"] < pd.Timedelta(\"5 min\"): # if the portion is less than 5 minutes, skip it\n",
    "            continue\n",
    "        \n",
    "        # Extract systolic peaks from the quiet PPG signal\n",
    "        ppg_quiet = ppg_night.loc[(quiet_period[\"start\"]+td):(quiet_period[\"end\"]-td)]\n",
    "        _, peaks = ppg_findpeaks_charlton(ppg_quiet[\"ppg\"].values, sampling_rate=64)\n",
    "        t_peaks = ppg.index.to_series().values[peaks]\n",
    "        ibi = np.diff(t_peaks).astype('timedelta64[ns]').astype('float64') / 1000000000 # seconds\n",
    "        ibi = np.insert(ibi, 0, 0, axis = 0)\n",
    "        ibi[0] = np.mean(ibi[1:10])\n",
    "        ibi = pd.Series(ibi, index = t_peaks)\n",
    "        # plt.plot(ppg_quiet)\n",
    "        # plt.plot(ppg_quiet.index[peaks], ppg_quiet[\"ppg\"].values[peaks], 'ro')\n",
    "\n",
    "        # Kubios\n",
    "        artifacts, env_diff_corrected = signal_fixpeaks(ibi.values, 64, iterative = False)\n",
    "        artifacts_all = np.concatenate((artifacts[\"ectopic\"], artifacts[\"missed\"], artifacts[\"extra\"], artifacts[\"longshort\"]))\n",
    "        ibi[ibi.index[artifacts_all.astype(int)]] = np.nan\n",
    "        ibi_clean = ibi.interpolate(method=\"linear\")\n",
    "\n",
    "        ##### HRV features #####\n",
    "        ppi = ibi_clean.values * 1000 # ms\n",
    "        diff_ppi = np.diff(ppi)\n",
    "\n",
    "        # RMSSD\n",
    "        rmssd = np.sqrt(np.mean(diff_ppi**2))\n",
    "        \n",
    "        # SDNN\n",
    "        sdnn = np.std(ppi, ddof=1)\n",
    "\n",
    "        # HRF\n",
    "        PIP = compute_HRF(ppi)\n",
    "\n",
    "        HRV_df.append({\"day\": i+1, \"start\": quiet_period[\"start\"], \"end\": quiet_period[\"end\"], \"rmssd\": rmssd, \"sdnn\": sdnn, \"PIP\": PIP, \"middle\": quiet_period[\"start\"] + (quiet_period[\"end\"] - quiet_period[\"start\"])/2})\n",
    "\n",
    "HRV_df = pd.DataFrame(HRV_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = pd.Timedelta(\"5 min\")  # 5-minute window\n",
    "step_size = pd.Timedelta(\"30 s\")  # 30-second overlap\n",
    "\n",
    "HRV_df = []  # Reset HRV storage\n",
    "\n",
    "ibi_quiet_all = []\n",
    "\n",
    "for i, (start_sleep, end_sleep) in enumerate(start_end_sleep):  # for each night\n",
    "\n",
    "    HRV[f\"day {i+1}\"] = []\n",
    "\n",
    "    acc_night = compute_acc_SMV(acc_df.loc[start_sleep:end_sleep])\n",
    "    ppg_night = ppg_df.loc[start_sleep:end_sleep]\n",
    "\n",
    "    # Detect wrist accelerometer bursts\n",
    "    bursts = detect_bursts(acc_night, sampling_rate=64, alfa=thresold_bursts/1000)\n",
    "\n",
    "    # Extract quiet periods (no movement of the wrist)\n",
    "    quiet_periods = pd.DataFrame()\n",
    "    quiet_periods[\"start\"] = bursts[\"end\"].iloc[:-1].reset_index(drop=True)\n",
    "    quiet_periods[\"end\"] = bursts[\"start\"].iloc[1:].reset_index(drop=True)\n",
    "\n",
    "    for _, quiet_period in quiet_periods.iterrows():  # for each quiet period\n",
    "\n",
    "        total_duration = quiet_period[\"end\"] - quiet_period[\"start\"]\n",
    "        if total_duration < window_length:  # If the whole period is shorter than 5 min, skip it\n",
    "            continue\n",
    "\n",
    "         # Extract systolic peaks from the quiet PPG signal\n",
    "        _, peaks = ppg_findpeaks_charlton(ppg_quiet[\"ppg\"].values, sampling_rate=64)\n",
    "        t_peaks = ppg_quiet.index.to_series().values[peaks]\n",
    "        ibi = np.diff(t_peaks).astype('timedelta64[ns]').astype('float64') / 1e9  # seconds\n",
    "        ibi = np.insert(ibi, 0, np.mean(ibi[1:10]), axis=0)  # Set first value as mean of next 10\n",
    "        ibi = pd.Series(ibi, index=t_peaks)\n",
    "\n",
    "        # Kubios artifact correction\n",
    "        artifacts, env_diff_corrected = signal_fixpeaks(ibi.values, 64, iterative=False)\n",
    "        artifacts_all = np.concatenate((artifacts[\"ectopic\"], artifacts[\"missed\"], artifacts[\"extra\"], artifacts[\"longshort\"]))\n",
    "        ibi[ibi.index[artifacts_all.astype(int)]] = np.nan\n",
    "        ibi_clean = ibi.interpolate(method=\"linear\")\n",
    "        # Generate overlapping windows of 5 minutes with 30-second overlap\n",
    "        current_start = quiet_period[\"start\"]\n",
    "        \n",
    "        while current_start + window_length <= quiet_period[\"end\"]:\n",
    "            current_end = current_start + window_length\n",
    "\n",
    "            # Extract PPG in the current window\n",
    "            ppg_quiet = ppg_night.loc[(current_start+td):(current_end-td)]\n",
    "            if len(ppg_quiet) < 100:  # Skip very short segments\n",
    "                current_start += step_size\n",
    "                continue\n",
    "\n",
    "            # Extract systolic peaks from the quiet PPG signal\n",
    "            _, peaks = ppg_findpeaks_charlton(ppg_quiet[\"ppg\"].values, sampling_rate=64)\n",
    "            t_peaks = ppg_quiet.index.to_series().values[peaks]\n",
    "            ibi = np.diff(t_peaks).astype('timedelta64[ns]').astype('float64') / 1e9  # seconds\n",
    "            ibi = np.insert(ibi, 0, np.mean(ibi[1:10]), axis=0)  # Set first value as mean of next 10\n",
    "            ibi = pd.Series(ibi, index=t_peaks)\n",
    "\n",
    "            # Kubios artifact correction\n",
    "            artifacts, env_diff_corrected = signal_fixpeaks(ibi.values, 64, iterative=False)\n",
    "            artifacts_all = np.concatenate((artifacts[\"ectopic\"], artifacts[\"missed\"], artifacts[\"extra\"], artifacts[\"longshort\"]))\n",
    "            ibi[ibi.index[artifacts_all.astype(int)]] = np.nan\n",
    "            ibi_clean = ibi.interpolate(method=\"linear\")\n",
    "\n",
    "            # HRV Features\n",
    "            ppi = ibi_clean.values * 1000  # Convert to ms\n",
    "            diff_ppi = np.diff(ppi)\n",
    "\n",
    "            rmssd = np.sqrt(np.mean(diff_ppi**2))  # RMSSD\n",
    "            sdnn = np.std(ppi, ddof=1)  # SDNN\n",
    "            PIP = compute_HRF(ppi)  # Custom HRF computation\n",
    "\n",
    "            HRV_df.append({\n",
    "                \"day\": i+1, \n",
    "                \"window_start\": current_start, \n",
    "                \"window_end\": current_end, \n",
    "                \"rmssd\": rmssd, \n",
    "                \"sdnn\": sdnn, \n",
    "                \"PIP\": PIP, \n",
    "                \"middle\": current_start + window_length / 2\n",
    "            })\n",
    "\n",
    "            current_start += step_size  # Move to next overlapping window\n",
    "\n",
    "        ibi_quiet_all.append(ibi)\n",
    "\n",
    "HRV_df = pd.DataFrame(HRV_df)\n",
    "ibi_quiet_df = pd.concat(ibi_quiet_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibi_quiet_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>window_start</th>\n",
       "      <th>window_end</th>\n",
       "      <th>rmssd</th>\n",
       "      <th>sdnn</th>\n",
       "      <th>PIP</th>\n",
       "      <th>middle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-05-21 01:13:01.088835001</td>\n",
       "      <td>2024-05-21 01:18:01.088835001</td>\n",
       "      <td>51.686506</td>\n",
       "      <td>64.647549</td>\n",
       "      <td>0.684825</td>\n",
       "      <td>2024-05-21 01:15:31.088835001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-05-21 01:13:31.088835001</td>\n",
       "      <td>2024-05-21 01:18:31.088835001</td>\n",
       "      <td>56.308176</td>\n",
       "      <td>62.170712</td>\n",
       "      <td>0.683794</td>\n",
       "      <td>2024-05-21 01:16:01.088835001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-05-21 01:14:01.088835001</td>\n",
       "      <td>2024-05-21 01:19:01.088835001</td>\n",
       "      <td>65.587052</td>\n",
       "      <td>74.812008</td>\n",
       "      <td>0.681275</td>\n",
       "      <td>2024-05-21 01:16:31.088835001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-05-21 01:14:31.088835001</td>\n",
       "      <td>2024-05-21 01:19:31.088835001</td>\n",
       "      <td>68.600474</td>\n",
       "      <td>79.210685</td>\n",
       "      <td>0.677419</td>\n",
       "      <td>2024-05-21 01:17:01.088835001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-05-21 01:15:01.088835001</td>\n",
       "      <td>2024-05-21 01:20:01.088835001</td>\n",
       "      <td>71.855343</td>\n",
       "      <td>77.171793</td>\n",
       "      <td>0.680328</td>\n",
       "      <td>2024-05-21 01:17:31.088835001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2368</th>\n",
       "      <td>6</td>\n",
       "      <td>2024-05-27 05:43:28.464230061</td>\n",
       "      <td>2024-05-27 05:48:28.464230061</td>\n",
       "      <td>30.272144</td>\n",
       "      <td>46.502956</td>\n",
       "      <td>0.663082</td>\n",
       "      <td>2024-05-27 05:45:58.464230061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2369</th>\n",
       "      <td>6</td>\n",
       "      <td>2024-05-27 05:43:58.464230061</td>\n",
       "      <td>2024-05-27 05:48:58.464230061</td>\n",
       "      <td>29.064951</td>\n",
       "      <td>48.197264</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2024-05-27 05:46:28.464230061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2370</th>\n",
       "      <td>6</td>\n",
       "      <td>2024-05-27 05:44:28.464230061</td>\n",
       "      <td>2024-05-27 05:49:28.464230061</td>\n",
       "      <td>27.299414</td>\n",
       "      <td>53.358816</td>\n",
       "      <td>0.699301</td>\n",
       "      <td>2024-05-27 05:46:58.464230061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>6</td>\n",
       "      <td>2024-05-27 05:49:44.401791096</td>\n",
       "      <td>2024-05-27 05:54:44.401791096</td>\n",
       "      <td>42.737534</td>\n",
       "      <td>52.782592</td>\n",
       "      <td>0.621528</td>\n",
       "      <td>2024-05-27 05:52:14.401791096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372</th>\n",
       "      <td>6</td>\n",
       "      <td>2024-05-27 05:54:58.808041096</td>\n",
       "      <td>2024-05-27 05:59:58.808041096</td>\n",
       "      <td>36.875507</td>\n",
       "      <td>58.651174</td>\n",
       "      <td>0.686833</td>\n",
       "      <td>2024-05-27 05:57:28.808041096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2373 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      day                  window_start                    window_end  \\\n",
       "0       1 2024-05-21 01:13:01.088835001 2024-05-21 01:18:01.088835001   \n",
       "1       1 2024-05-21 01:13:31.088835001 2024-05-21 01:18:31.088835001   \n",
       "2       1 2024-05-21 01:14:01.088835001 2024-05-21 01:19:01.088835001   \n",
       "3       1 2024-05-21 01:14:31.088835001 2024-05-21 01:19:31.088835001   \n",
       "4       1 2024-05-21 01:15:01.088835001 2024-05-21 01:20:01.088835001   \n",
       "...   ...                           ...                           ...   \n",
       "2368    6 2024-05-27 05:43:28.464230061 2024-05-27 05:48:28.464230061   \n",
       "2369    6 2024-05-27 05:43:58.464230061 2024-05-27 05:48:58.464230061   \n",
       "2370    6 2024-05-27 05:44:28.464230061 2024-05-27 05:49:28.464230061   \n",
       "2371    6 2024-05-27 05:49:44.401791096 2024-05-27 05:54:44.401791096   \n",
       "2372    6 2024-05-27 05:54:58.808041096 2024-05-27 05:59:58.808041096   \n",
       "\n",
       "          rmssd       sdnn       PIP                        middle  \n",
       "0     51.686506  64.647549  0.684825 2024-05-21 01:15:31.088835001  \n",
       "1     56.308176  62.170712  0.683794 2024-05-21 01:16:01.088835001  \n",
       "2     65.587052  74.812008  0.681275 2024-05-21 01:16:31.088835001  \n",
       "3     68.600474  79.210685  0.677419 2024-05-21 01:17:01.088835001  \n",
       "4     71.855343  77.171793  0.680328 2024-05-21 01:17:31.088835001  \n",
       "...         ...        ...       ...                           ...  \n",
       "2368  30.272144  46.502956  0.663082 2024-05-27 05:45:58.464230061  \n",
       "2369  29.064951  48.197264  0.666667 2024-05-27 05:46:28.464230061  \n",
       "2370  27.299414  53.358816  0.699301 2024-05-27 05:46:58.464230061  \n",
       "2371  42.737534  52.782592  0.621528 2024-05-27 05:52:14.401791096  \n",
       "2372  36.875507  58.651174  0.686833 2024-05-27 05:57:28.808041096  \n",
       "\n",
       "[2373 rows x 7 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HRV_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x451162b70>]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(19, 11))\n",
    "\n",
    "days = HRV_df[\"day\"].unique()\n",
    "\n",
    "plt.figure(figsize=(19, 11))\n",
    "plt.subplot(2, 1, 1)\n",
    "for day in days:\n",
    "    HRV_day = HRV_df[HRV_df[\"day\"] == day]\n",
    "    plt.plot(HRV_day[\"middle\"], HRV_day[\"rmssd\"], '-o', label=f\"day {day}\")\n",
    "\n",
    "plt.legend()\n",
    "plt.subplot(2, 1, 2, sharex = plt.subplot(2, 1, 1))\n",
    "plt.plot(ibi_quiet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2024-05-20 01:02:45')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime(str(acc_df.index[0].date()) + \" \" + output_GGIR[\"sleeponset_ts\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_GGIR_path = \"/Users/augenpro/Documents/Empatica/data_sara/data/GGIR_output/\"\n",
    "\n",
    "output_GGIR = pd.read_csv(output_GGIR_path + \"part4_nightsummary_sleep_full.csv\")\n",
    "\n",
    "tst = output_GGIR[\"SleepDurationInSpt\"].values[0]\n",
    "\n",
    "waso = output_GGIR[\"WASO\"].values[0]\n",
    "\n",
    "SE = output_GGIR[\"SE\"]\n",
    "\n",
    "SE = tst / (tst + waso)\n",
    "\n",
    "# Sleep and wake onsets\n",
    "if output_GGIR[\"sleeponset_ts\"].iloc[0][0] == '0':\n",
    "    sleep_onset = pd.to_datetime(str(acc_norm.index[0].date() + pd.Timedelta(\"1d\")) + \" \" + output_GGIR[\"sleeponset_ts\"].iloc[0])\n",
    "else: \n",
    "    sleep_onset = pd.to_datetime(str(acc_norm.index[0].date())  + \" \" + output_GGIR[\"sleeponset_ts\"].iloc[0])\n",
    "\n",
    "wake_onset = pd.to_datetime(str(acc_norm.index[0].date() + pd.Timedelta(\"1d\")) + \" \" + output_GGIR[\"wakeup_ts\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-05-20 13:02:55.531980990</th>\n",
       "      <td>-0.068359</td>\n",
       "      <td>0.408691</td>\n",
       "      <td>0.931152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-20 13:02:55.547605991</th>\n",
       "      <td>-0.068359</td>\n",
       "      <td>0.403320</td>\n",
       "      <td>0.930176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-20 13:02:55.563230991</th>\n",
       "      <td>-0.071289</td>\n",
       "      <td>0.397949</td>\n",
       "      <td>0.919434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-20 13:02:55.578855991</th>\n",
       "      <td>-0.069824</td>\n",
       "      <td>0.400879</td>\n",
       "      <td>0.925781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-20 13:02:55.594480991</th>\n",
       "      <td>-0.064453</td>\n",
       "      <td>0.409668</td>\n",
       "      <td>0.923340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      x         y         z\n",
       "2024-05-20 13:02:55.531980990 -0.068359  0.408691  0.931152\n",
       "2024-05-20 13:02:55.547605991 -0.068359  0.403320  0.930176\n",
       "2024-05-20 13:02:55.563230991 -0.071289  0.397949  0.919434\n",
       "2024-05-20 13:02:55.578855991 -0.069824  0.400879  0.925781\n",
       "2024-05-20 13:02:55.594480991 -0.064453  0.409668  0.923340"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc.index.is_monotonic_increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DARE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
